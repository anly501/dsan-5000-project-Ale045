[
  {
    "objectID": "naive_bayes.html",
    "href": "naive_bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes is a classification algorithm that is suitable for binary and multiclass classification. It is based on the Bayes theorem and assumes that the features are independent of each other which is why it is called naive. Though the assumption does not hold true for most statistical data, the algorithm still achive good performace in many cases."
  },
  {
    "objectID": "naive_bayes.html#introduction-to-naive-bayes",
    "href": "naive_bayes.html#introduction-to-naive-bayes",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes is a classification algorithm that is suitable for binary and multiclass classification. It is based on the Bayes theorem and assumes that the features are independent of each other which is why it is called naive. Though the assumption does not hold true for most statistical data, the algorithm still achive good performace in many cases."
  },
  {
    "objectID": "naive_bayes.html#bayes-theorem",
    "href": "naive_bayes.html#bayes-theorem",
    "title": "Naive Bayes",
    "section": "Bayes Theorem",
    "text": "Bayes Theorem\nBayes theorem is a probabability theory which update the prior belife by incorporating new evidence. The formula is as follows:\n\\[P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\\]\nwhere \\(P(A|B)\\) is the posterior probability, \\(P(B|A)\\) is the likelihood, \\(P(A)\\) is the prior probability and \\(P(B)\\) is the evidence.\nIn a Naive Bayes classifier, each time we use this formula to calculate the posterior probability of each class and the class with the highest probability is the predicted class."
  },
  {
    "objectID": "naive_bayes.html#aim",
    "href": "naive_bayes.html#aim",
    "title": "Naive Bayes",
    "section": "Aim",
    "text": "Aim\nIn this project, I will propse two Naive Bayes classifer which one of them will be using the music’s acoustic features to predcit the year range of the song and the other using the lyric of the song to predict the valence stage."
  },
  {
    "objectID": "naive_bayes.html#different-types-of-naive-bayes-classifiers",
    "href": "naive_bayes.html#different-types-of-naive-bayes-classifiers",
    "title": "Naive Bayes",
    "section": "Different types of Naive Bayes Classifiers",
    "text": "Different types of Naive Bayes Classifiers\n\nGaussian Naive Bayes: It is used in classification and assumes that the features follow a normal distribution.This is normally used when dealing with contiouns data.\nMultinomial Naive Bayes: It is used in classification and assumes that the features follow a multinomial distribution. This is normally used when dealing with discrete data.\nBernoulli Naive Bayes: It is used in classification and assumes that the features follow a Bernoulli distribution. This is normally used when dealing with binary data."
  },
  {
    "objectID": "naive_bayes_record.html",
    "href": "naive_bayes_record.html",
    "title": "Naive Bayes on Record Data",
    "section": "",
    "text": "The aim here is to use Naive Bayes to predict in which range of year the song is from base on it’s acoustic features.\n\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nimport random\nfrom sklearn import metrics\n\n\nimport time\n\n\n# load data\ndata = pd.read_csv('../data/01-modified-data/top50MusicFrom2010-2019_cleaned.csv')\n\n\n# Creaet a training method\ndef train_and_test(x_train, y_train, x_test, y_test, i_print=False):\n\n    gnb = GaussianNB()\n    start = time.process_time()\n    gnb.fit(x_train, y_train)\n    end = time.process_time()\n    time_train = end - start\n    start = time.process_time()\n    y_pred = gnb.predict(x_test)\n    end = time.process_time()\n    time_eval = end - start\n    accuracy = accuracy_score(y_test, y_pred)\n    accuracy_train = accuracy_score(y_train, gnb.predict(x_train))\n    # f1 = f1_score(y_test, y_pred, pos_label='2010-2014')\n    if i_print:\n        print(accuracy*100, accuracy_train*100) \n    \n    return accuracy_train, accuracy, time_train, time_eval\n\n\nX = data.drop(['year', 'year_group', 'genre', 'title', 'artist'], axis=1)\ny = data['year_group'].values\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)\n\n\ntrain_and_test(X_train, y_train, X_test, y_test, i_print=True)\n\n59.50413223140496 65.56016597510373\n\n\n(0.6556016597510373,\n 0.5950413223140496,\n 0.004610999999997034,\n 0.0021029999999981897)\n\n\nWe can see the initial testing result is 59% and it’s not good enough. We will try to improve it using feature selection.\n\nN=X.shape[0]\nl = [*range(N)]       # indices\ncut = int(0.7 * N)    # 80% of the list\nrandom.shuffle(l)     # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:]  # last 20% of shuffled list"
  },
  {
    "objectID": "naive_bayes_record.html#implementation",
    "href": "naive_bayes_record.html#implementation",
    "title": "Naive Bayes on Record Data",
    "section": "",
    "text": "The aim here is to use Naive Bayes to predict in which range of year the song is from base on it’s acoustic features.\n\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nimport random\nfrom sklearn import metrics\n\n\nimport time\n\n\n# load data\ndata = pd.read_csv('../data/01-modified-data/top50MusicFrom2010-2019_cleaned.csv')\n\n\n# Creaet a training method\ndef train_and_test(x_train, y_train, x_test, y_test, i_print=False):\n\n    gnb = GaussianNB()\n    start = time.process_time()\n    gnb.fit(x_train, y_train)\n    end = time.process_time()\n    time_train = end - start\n    start = time.process_time()\n    y_pred = gnb.predict(x_test)\n    end = time.process_time()\n    time_eval = end - start\n    accuracy = accuracy_score(y_test, y_pred)\n    accuracy_train = accuracy_score(y_train, gnb.predict(x_train))\n    # f1 = f1_score(y_test, y_pred, pos_label='2010-2014')\n    if i_print:\n        print(accuracy*100, accuracy_train*100) \n    \n    return accuracy_train, accuracy, time_train, time_eval\n\n\nX = data.drop(['year', 'year_group', 'genre', 'title', 'artist'], axis=1)\ny = data['year_group'].values\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)\n\n\ntrain_and_test(X_train, y_train, X_test, y_test, i_print=True)\n\n59.50413223140496 65.56016597510373\n\n\n(0.6556016597510373,\n 0.5950413223140496,\n 0.004610999999997034,\n 0.0021029999999981897)\n\n\nWe can see the initial testing result is 59% and it’s not good enough. We will try to improve it using feature selection.\n\nN=X.shape[0]\nl = [*range(N)]       # indices\ncut = int(0.7 * N)    # 80% of the list\nrandom.shuffle(l)     # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:]  # last 20% of shuffled list"
  },
  {
    "objectID": "naive_bayes_record.html#feature-selection-for-record-data-using-variance-threshold",
    "href": "naive_bayes_record.html#feature-selection-for-record-data-using-variance-threshold",
    "title": "Naive Bayes on Record Data",
    "section": "Feature Selection for record data (Using Variance Threshold)",
    "text": "Feature Selection for record data (Using Variance Threshold)\n\n# VARIANCE THRESHOLD SEARCH\n\nfrom sklearn.feature_selection import VarianceThreshold\n\nx_var=np.var(X,axis=0)\n# DEFINE GRID OF THRESHOLDS \nnum_thresholds=10\nthresholds=np.linspace(np.min(x_var),np.max(x_var),num_thresholds)\n\n# DOESN\"T WORK WELL WITH EDGE VALUES (ZERO VAR)\nthresholds=thresholds[1:-2]; #print(thresholds)\n\n# INITIALIZE ARRAYS\nnum_features=[]\ntrain_accuracies=[]\ntest_accuracies=[]\ntrain_time = []\neval_time = []\n\n#FULL TRAINING SET\n\n(acc_train,acc_test, time_train, time_eval)=train_and_test(X_train,y_train,X_test,y_test,i_print=True)\nnum_features.append(X.shape[1])\ntrain_accuracies.append(acc_train)\ntest_accuracies.append(acc_test)\ntrain_time.append(time_train)\neval_time.append(time_eval)\n\n# SEARCH FOR OPTIMAL THRESHOLD\nfor THRESHOLD in thresholds:\n    feature_selector = VarianceThreshold(threshold=THRESHOLD)\n    xtmp=feature_selector.fit_transform(X)\n    print(THRESHOLD, xtmp.shape[1])\n\n    x_train=xtmp[train_index]; y_train=y[train_index]\n    x_test=xtmp[test_index]; y_test=y[test_index]\n\n\n    (acc_train,acc_test, time_train, time_eval)=train_and_test(x_train,y_train,x_test,y_test,i_print=False)\n             \n    #RECORD \n    num_features.append(xtmp.shape[1])\n    train_accuracies.append(acc_train)\n    test_accuracies.append(acc_test)\n    train_time.append(time_train)\n    eval_time.append(time_eval)\n\n\n59.50413223140496 65.56016597510373\n136.16184967918835 7\n264.507765209332 5\n392.8536807394756 4\n521.1995962696192 2\n649.5455117997628 1\n777.8914273299064 1\n906.23734286005 1\n\n\n\neval_time\n\n[0.0005700000000032901,\n 0.00014000000000180535,\n 0.00015100000000245473,\n 0.00010799999999733245,\n 0.00011200000000144428,\n 9.800000000126374e-05,\n 0.00010000000000331966,\n 9.299999999967667e-05]\n\n\n\nnum_features\n\n[9, 7, 5, 4, 2, 1, 1, 1]\n\n\n\n#UTILITY FUNCTION TO PLOT RESULTS\ndef plot_results():\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (red) and Test (blue)')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.show()\n\n\nmax(test_accuracies)\n\n0.6574585635359116\n\n\n\nplot_results()\n\n\n\n\n\n\n\n\n\n\n\nThe first graph shows the performance of the model’s trend during the added features. Both the training and testing accuarcy were at peak when the feature number is 9 (all of them), which should be the best number of features to use. The best accuarcy it can give is about 0.68. The interesting thing is that the accuarcy of training are lower than the testing accuarcy. It could be a result of underfitting due to the lack of features or simple nature of naive bayes.\nThis graph shows the time used for training and testing related to the number of the features. It can be observed that after the feature number is greater than 7 the time consumption for training and testing increased dramatically. Therefore, 7 is the best number for reducing time cost.\nThis graph shown how the difference of the accuarcy between the training and testing change according to the number of features. It can be observed that the difference is quite low when the feature number is 7 and and 1."
  },
  {
    "objectID": "naive_bayes_record.html#feature-selection-for-record-data-using-iterative-feature-selection",
    "href": "naive_bayes_record.html#feature-selection-for-record-data-using-iterative-feature-selection",
    "title": "Naive Bayes on Record Data",
    "section": "Feature Selection for record data (Using iterative feature selection)",
    "text": "Feature Selection for record data (Using iterative feature selection)\n\nimport itertools\n\n\ndef maximize_CFS(x,y):\n     \n     df_x = x\n     column_names = df_x.columns\n\n     N=X.shape[0]\n     l = [*range(N)]       # indices\n     cut = int(0.7 * N)    # 80% of the list\n     random.shuffle(l)     # randomize\n     train_index = l[:cut] # first 80% of shuffled list\n     test_index = l[cut:]  # last 20% of shuffled list\n\n     max_accuracy = 0\n     y_train=y[train_index]\n     y_test=y[test_index]\n     \n     for L in range(1,len(column_names) + 1):\n          for subset in itertools.combinations(column_names, L):\n               # print(df_x[list(subset)].values.shape)\n               temp = df_x[list(subset)]\n               x_train=temp.iloc[train_index]\n               x_test=temp.iloc[test_index]\n               _, accuracy, _, _ = train_and_test(x_train, y_train, x_test, y_test)\n               if accuracy &gt; max_accuracy:\n                    max_accuracy = accuracy\n                    max_subset = list(subset)\n                    print(f'found new max: {max_accuracy}  optimal features = {max_subset} \\niteration= {L}, accuarcy = {accuracy}')\n          \n     \n     return max_accuracy, max_subset, y_test, x_test, y_train, x_train\n\n\nacc, subset, y_test, x_test, y_train, x_train = maximize_CFS(X,y)\n\nfound new max: 0.585635359116022  optimal features = ['beat'] \niteration= 1, accuarcy = 0.585635359116022\nfound new max: 0.5966850828729282  optimal features = ['danceability'] \niteration= 1, accuarcy = 0.5966850828729282\nfound new max: 0.6408839779005525  optimal features = ['popularity'] \niteration= 1, accuarcy = 0.6408839779005525\nfound new max: 0.6519337016574586  optimal features = ['duration', 'popularity'] \niteration= 2, accuarcy = 0.6519337016574586\nfound new max: 0.6574585635359116  optimal features = ['beat', 'duration', 'popularity'] \niteration= 3, accuarcy = 0.6574585635359116\nfound new max: 0.6629834254143646  optimal features = ['energy', 'duration', 'popularity'] \niteration= 3, accuarcy = 0.6629834254143646\nfound new max: 0.6685082872928176  optimal features = ['beat', 'danceability', 'duration', 'popularity'] \niteration= 4, accuarcy = 0.6685082872928176\nfound new max: 0.6740331491712708  optimal features = ['beat', 'valence', 'duration', 'popularity'] \niteration= 4, accuarcy = 0.6740331491712708\nfound new max: 0.6961325966850829  optimal features = ['danceability', 'valence', 'duration', 'popularity'] \niteration= 4, accuarcy = 0.6961325966850829\nfound new max: 0.7071823204419889  optimal features = ['beat', 'danceability', 'valence', 'duration', 'popularity'] \niteration= 5, accuarcy = 0.7071823204419889\n\n\n\nacc\n\n0.7071823204419889\n\n\n\nsubset\n\n['beat', 'danceability', 'valence', 'duration', 'popularity']\n\n\nUsing the iterative feature selection, the best features combination were selected and the accuarcy is higher than using the variance threshold in the last case. So I will use this as the final result."
  },
  {
    "objectID": "naive_bayes_record.html#final-result",
    "href": "naive_bayes_record.html#final-result",
    "title": "Naive Bayes on Record Data",
    "section": "Final Result",
    "text": "Final Result\n\ndef generate_final_result(x_train, y_train, x_test, y_test):\n\n    gnb = GaussianNB()\n    gnb.fit(x_train, y_train)\n    y_pred = gnb.predict(x_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    accuracy_train = accuracy_score(y_train, gnb.predict(x_train))\n    f1 = f1_score(y_test, y_pred, pos_label='2010-2014')\n\n    \n    return f1, accuracy, accuracy_train, y_pred, y_test\n\n\nf1, accuarcy, accuracy_train, y_pred, y_test = generate_final_result(x_train[subset], y_train, x_test[subset], y_test)\n\n\nf1\n\n0.7665198237885462\n\n\n\naccuarcy\n\n0.7071823204419889\n\n\n\nconf_matrix = metrics.confusion_matrix(y_test, y_pred, labels=['2010-2014', '2015-2019'])\n\nplt.title(\"Confusion matrix\")\naxis = sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\naxis.set_xticklabels(['2010-2014', '2015-2019'])\naxis.set_yticklabels(['2010-2014', '2015-2019'])\naxis.set(xlabel=\"Predicted\", ylabel=\"True\")\nplt.show()\n\n\n\n\nAccording to the confusion matrix, this model would violently biased to the case of 2010-2014. This could be a result of the lack of data in general. Though the accuracy is not bad but the model is not very useful in practice because of the high bias."
  },
  {
    "objectID": "naive_bayes_record.html#discussion",
    "href": "naive_bayes_record.html#discussion",
    "title": "Naive Bayes on Record Data",
    "section": "Discussion",
    "text": "Discussion\n\nThe modle was trained using the sklern inbulit GaussianNB function. Using the fit method to train on the training dataset and predict method to test on the test set.\nThe model has a good accuarcy but a terrible recall for class 2010-2014. So although the model can predict the year of the song. The F1 score for class 2010-2014 is decent but for another class is not pretty satisfying.This could be the result of the lack of data in general.\nOverfitting is when the model is too complex to learn even the noise of the training data, while underfitting is when the model is too simple and fail to learn the pattern at all. While overfitting has a high variance and low bias while overfitting could have low variance and high bias. In this case, due to the high bias, the model is more likely to be underfitting.\nThe project finds that there is a possible way to use the acoustic features of a song to roughly decide its year of release. If there is a need for a report, this should be documented using like a R markdown or Jupyter notebook so the result is reproduceable. What should be reported include how the modle was trained, what is the dataset, the result and the limitation."
  },
  {
    "objectID": "naive_bayes_record.html#conclusion",
    "href": "naive_bayes_record.html#conclusion",
    "title": "Naive Bayes on Record Data",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, though the model get a decent accuracy score, the overly high bias makes it not practicel when dealing with certain classes. But consider the limited amount of data, this model might be able to be improved if more time were given and more data is acquired."
  },
  {
    "objectID": "nb_text.html",
    "href": "nb_text.html",
    "title": "Naive Bayes on Text Data",
    "section": "",
    "text": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport random\nfrom sklearn.metrics import f1_score\nfrom sklearn import metrics\nimport seaborn as sns\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Prepare the dataset\ndf_text = pd.read_csv('../data/01-modified-data/genius_lyrics_cleaned.csv')\ndf_track = pd.read_csv('../data/01-modified-data/top50MusicFrom2010-2019_cleaned.csv')\n\n\n# Merge the dataset\ndf = pd.merge(df_text, df_track, left_on=['track_name','artist_name'], right_on=['title', 'artist'], how='inner')\n\n\ndf = df.dropna()\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntrack_name\nartist_name\naa\naaliyah\naap\naaron\nable\naboutyou\nabsolutely\nabstract\n...\nbeat_y\nenergy_y\ndanceability\nloudness\nvalence\nduration\nacousticness\nspeechiness\npopularity\nyear_group\n\n\n\n\n0\nHey, Soul Sister\nTrain\n0\n0\n0\n0\n0\n0\n0\n0\n...\n97\n89\n67\n-4\n80\n217\n19\n4\n83\n2010-2014\n\n\n1\nLove The Way You Lie\nEminem\n0\n0\n0\n0\n0\n0\n0\n0\n...\n87\n93\n75\n-5\n64\n263\n24\n23\n82\n2010-2014\n\n\n2\nBad Romance\nLady Gaga\n0\n0\n0\n0\n0\n0\n0\n0\n...\n119\n92\n70\n-4\n71\n295\n0\n4\n79\n2010-2014\n\n\n3\nJust the Way You Are\nBruno Mars\n0\n0\n0\n0\n0\n0\n0\n0\n...\n109\n84\n64\n-5\n43\n221\n2\n4\n78\n2010-2014\n\n\n4\nJust the Way You Are\nBruno Mars\n0\n0\n0\n0\n0\n0\n0\n0\n...\n109\n84\n64\n-5\n43\n221\n2\n4\n78\n2010-2014\n\n\n\n\n5 rows × 5275 columns\n\n\n\n\ndf['valence_cat'] = df['valence'].apply(lambda x: 1 if x &gt; 50 else 0)\n\n\nprior_0 = df['year_group'].value_counts()[0]/len(df)\nprior_1 = df['year_group'].value_counts()[1]/len(df)\n\n\n# leave only the columns we need\ndf = df.drop(['danceability', 'energy_x', 'energy_y', 'loudness', 'speechiness',\n       'acousticness', 'beat_y','artist_name', 'title', 'artist_y', 'genre', 'year_y', 'title', 'track_name', 'year_group'], axis=1)\n\n\nx = df.drop(['valence_cat'], axis=1)\n\n\ny = df['valence_cat']\n\n\nimport random\nN= x.shape[0]\nl = [*range(N)]     # indices\ncut = int(0.8 * N) #80% of the list\nrandom.shuffle(l)   # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:] # last 20% of shuffled list\n\nprint(train_index[0:10])\nprint(test_index[0:10])\n\n[268, 412, 426, 402, 310, 142, 466, 58, 274, 343]\n[53, 400, 106, 391, 232, 40, 286, 351, 20, 264]\n\n\n\nx.columns = range(x.columns.size)\nprint(x.head())\nprint(x.sum(axis=0))\nx=x.to_numpy()\ny = y.values\n\n   0     1     2     3     4     5     6     7     8     9     ...  5251  \\\n0     0     0     0     0     0     0     0     0     0     0  ...     0   \n1     0     0     0     0     0     0     0     0     0     0  ...     0   \n2     0     0     0     0     0     0     0     0     0     0  ...     0   \n3     0     0     0     0     0     0     0     0     0     0  ...     0   \n4     0     0     0     0     0     0     0     0     0     0  ...     0   \n\n   5252  5253  5254  5255  5256  5257  5258  5259  5260  \n0     0     0     0     0     0     0    80   217    83  \n1     0     0     0     0     0     0    64   263    82  \n2     0     0     0     0     0     0    71   295    79  \n3     0     0     0     0     0     0    43   221    78  \n4     0     0     0     0     0     0    43   221    78  \n\n[5 rows x 5261 columns]\n0           36\n1            5\n2           90\n3           13\n4            7\n         ...  \n5256         3\n5257        10\n5258     30244\n5259    130615\n5260     38386\nLength: 5261, dtype: int64\n\n\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_MNB_model(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # class_prior=[prior_0,prior_1]\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n    f1 = f1_score(y_test, yp_test)\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test,time_train,time_eval, f1)\n\n    return (acc_train,acc_test,time_train,time_eval)\n\n\n#TEST\nprint(type(x),type(y))\nprint(x.shape,y.shape)\n(acc_train,acc_test,time_train,time_eval)=train_MNB_model(x,y,i_print=True)\n\n&lt;class 'numpy.ndarray'&gt; &lt;class 'numpy.ndarray'&gt;\n(581, 5261) (581,)\n(581, 5261) (581,)\n94.39655172413794 71.7948717948718 0.007054999999999367 0.014889000000000152 0.7401574803149606\n\n\nHere we can see the training accuarcy is extremly high while the testing is consider lower. This could be a sign of overfitting. We can try to reduce the number of features to see if the accuracy will improve."
  },
  {
    "objectID": "nb_text.html#implementation",
    "href": "nb_text.html#implementation",
    "title": "Naive Bayes on Text Data",
    "section": "",
    "text": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport random\nfrom sklearn.metrics import f1_score\nfrom sklearn import metrics\nimport seaborn as sns\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Prepare the dataset\ndf_text = pd.read_csv('../data/01-modified-data/genius_lyrics_cleaned.csv')\ndf_track = pd.read_csv('../data/01-modified-data/top50MusicFrom2010-2019_cleaned.csv')\n\n\n# Merge the dataset\ndf = pd.merge(df_text, df_track, left_on=['track_name','artist_name'], right_on=['title', 'artist'], how='inner')\n\n\ndf = df.dropna()\n\n\ndf.head()\n\n\n\n\n\n\n\n\ntrack_name\nartist_name\naa\naaliyah\naap\naaron\nable\naboutyou\nabsolutely\nabstract\n...\nbeat_y\nenergy_y\ndanceability\nloudness\nvalence\nduration\nacousticness\nspeechiness\npopularity\nyear_group\n\n\n\n\n0\nHey, Soul Sister\nTrain\n0\n0\n0\n0\n0\n0\n0\n0\n...\n97\n89\n67\n-4\n80\n217\n19\n4\n83\n2010-2014\n\n\n1\nLove The Way You Lie\nEminem\n0\n0\n0\n0\n0\n0\n0\n0\n...\n87\n93\n75\n-5\n64\n263\n24\n23\n82\n2010-2014\n\n\n2\nBad Romance\nLady Gaga\n0\n0\n0\n0\n0\n0\n0\n0\n...\n119\n92\n70\n-4\n71\n295\n0\n4\n79\n2010-2014\n\n\n3\nJust the Way You Are\nBruno Mars\n0\n0\n0\n0\n0\n0\n0\n0\n...\n109\n84\n64\n-5\n43\n221\n2\n4\n78\n2010-2014\n\n\n4\nJust the Way You Are\nBruno Mars\n0\n0\n0\n0\n0\n0\n0\n0\n...\n109\n84\n64\n-5\n43\n221\n2\n4\n78\n2010-2014\n\n\n\n\n5 rows × 5275 columns\n\n\n\n\ndf['valence_cat'] = df['valence'].apply(lambda x: 1 if x &gt; 50 else 0)\n\n\nprior_0 = df['year_group'].value_counts()[0]/len(df)\nprior_1 = df['year_group'].value_counts()[1]/len(df)\n\n\n# leave only the columns we need\ndf = df.drop(['danceability', 'energy_x', 'energy_y', 'loudness', 'speechiness',\n       'acousticness', 'beat_y','artist_name', 'title', 'artist_y', 'genre', 'year_y', 'title', 'track_name', 'year_group'], axis=1)\n\n\nx = df.drop(['valence_cat'], axis=1)\n\n\ny = df['valence_cat']\n\n\nimport random\nN= x.shape[0]\nl = [*range(N)]     # indices\ncut = int(0.8 * N) #80% of the list\nrandom.shuffle(l)   # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:] # last 20% of shuffled list\n\nprint(train_index[0:10])\nprint(test_index[0:10])\n\n[268, 412, 426, 402, 310, 142, 466, 58, 274, 343]\n[53, 400, 106, 391, 232, 40, 286, 351, 20, 264]\n\n\n\nx.columns = range(x.columns.size)\nprint(x.head())\nprint(x.sum(axis=0))\nx=x.to_numpy()\ny = y.values\n\n   0     1     2     3     4     5     6     7     8     9     ...  5251  \\\n0     0     0     0     0     0     0     0     0     0     0  ...     0   \n1     0     0     0     0     0     0     0     0     0     0  ...     0   \n2     0     0     0     0     0     0     0     0     0     0  ...     0   \n3     0     0     0     0     0     0     0     0     0     0  ...     0   \n4     0     0     0     0     0     0     0     0     0     0  ...     0   \n\n   5252  5253  5254  5255  5256  5257  5258  5259  5260  \n0     0     0     0     0     0     0    80   217    83  \n1     0     0     0     0     0     0    64   263    82  \n2     0     0     0     0     0     0    71   295    79  \n3     0     0     0     0     0     0    43   221    78  \n4     0     0     0     0     0     0    43   221    78  \n\n[5 rows x 5261 columns]\n0           36\n1            5\n2           90\n3           13\n4            7\n         ...  \n5256         3\n5257        10\n5258     30244\n5259    130615\n5260     38386\nLength: 5261, dtype: int64\n\n\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_MNB_model(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # class_prior=[prior_0,prior_1]\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n    f1 = f1_score(y_test, yp_test)\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test,time_train,time_eval, f1)\n\n    return (acc_train,acc_test,time_train,time_eval)\n\n\n#TEST\nprint(type(x),type(y))\nprint(x.shape,y.shape)\n(acc_train,acc_test,time_train,time_eval)=train_MNB_model(x,y,i_print=True)\n\n&lt;class 'numpy.ndarray'&gt; &lt;class 'numpy.ndarray'&gt;\n(581, 5261) (581,)\n(581, 5261) (581,)\n94.39655172413794 71.7948717948718 0.007054999999999367 0.014889000000000152 0.7401574803149606\n\n\nHere we can see the training accuarcy is extremly high while the testing is consider lower. This could be a sign of overfitting. We can try to reduce the number of features to see if the accuracy will improve."
  },
  {
    "objectID": "nb_text.html#feature-selection-using-a-partial-grid-search",
    "href": "nb_text.html#feature-selection-using-a-partial-grid-search",
    "title": "Naive Bayes on Text Data",
    "section": "Feature Selection (Using a partial grid search)",
    "text": "Feature Selection (Using a partial grid search)\n\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        xtmp=x[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features.append(xtmp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search(num_runs=100, min_index=0, max_index=1000)\n\n# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\npartial_grid_search(num_runs=20, min_index=1000, max_index=5500)\n\n5 50 50 56.03448275862068 47.008547008547005\n10 100 100 60.129310344827594 50.427350427350426\n15 150 150 61.63793103448276 54.700854700854705\n20 200 200 62.71551724137932 52.991452991452995\n25 250 250 65.30172413793103 49.572649572649574\n30 300 300 67.88793103448276 57.26495726495726\n35 350 350 70.04310344827587 57.26495726495726\n40 400 400 71.98275862068965 61.53846153846154\n45 450 450 72.41379310344827 61.53846153846154\n50 500 500 74.13793103448276 61.53846153846154\n55 550 550 75.0 60.68376068376068\n60 600 600 74.78448275862068 62.39316239316239\n65 650 650 76.50862068965517 64.1025641025641\n70 700 700 75.21551724137932 64.1025641025641\n75 750 750 75.64655172413794 64.95726495726495\n80 800 800 75.21551724137932 64.95726495726495\n85 850 850 76.50862068965517 63.24786324786324\n90 900 900 77.15517241379311 64.95726495726495\n95 950 950 77.37068965517241 65.8119658119658\n100 1000 1000 77.80172413793103 65.8119658119658\n5 2125 2125 83.83620689655173 59.82905982905983\n10 3250 3250 85.99137931034483 55.55555555555556\n15 4375 4375 88.79310344827587 55.55555555555556\n20 5500 5261 94.39655172413794 71.7948717948718\n\n\n\ndef plot_results():\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (red) and Test (blue)')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.show()\n\n\nplot_results()\n\n\n\n\n\n\n\n\n\n\n\nThe graph shows the relationship between the model’s performance and the number of features used. We can see that thought the training accuracy is higher when the feature is more, the testing accuracy is becoming lower, indicating an overfitting. Until after the 5000 features was add then both the training and testing accuracy start to grow.\nIt shows the number of feature and the corrsponded time costing. We can see it is highly random when the feature number was small, but after 4000 features, the time cost starts to goes up relatively more stable.\nThis shows the number of feature and the corrseponded difference between training and testing accuracy. The difference keeps going up until reaching 5000 features, then it starts to go down, which align with the first graph.\n\n\nx_var=np.var(x,axis=0)\nprint(np.min(x_var))\nprint(np.max(x_var))\n\n0.005136849339823024\n1152.3875625442513"
  },
  {
    "objectID": "nb_text.html#feature-selection-using-variance-threshold",
    "href": "nb_text.html#feature-selection-using-variance-threshold",
    "title": "Naive Bayes on Text Data",
    "section": "Feature Selection (Using Variance Threshold)",
    "text": "Feature Selection (Using Variance Threshold)\n\nfrom sklearn.feature_selection import VarianceThreshold\n\n# DEFINE GRID OF THRESHOLDS \nnum_thresholds=30\nthresholds=np.linspace(np.min(x_var),np.max(x_var),num_thresholds)\n\n#DOESN\"T WORK WELL WITH EDGE VALUES \nthresholds=thresholds[1:-2]; #print(thresholds)\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\nmax_acc = 0\n# SEARCH FOR OPTIMAL THRESHOLD\nfor THRESHOLD in thresholds:\n    feature_selector = VarianceThreshold(threshold=THRESHOLD)\n    xtmp=feature_selector.fit_transform(x)\n    print(\"THRESHOLD =\",THRESHOLD, xtmp.shape[1])\n\n    (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n             \n    #RECORD \n    if acc_test &gt; max_acc:\n        max_acc = acc_test\n        x_best = xtmp\n    num_features.append(xtmp.shape[1])\n    train_accuracies.append(acc_train)\n    test_accuracies.append(acc_test)\n    train_time.append(time_train)\n    eval_time.append(time_eval)\n\nTHRESHOLD = 3.8592586744064166 75\nTHRESHOLD = 7.7133804994730095 40\nTHRESHOLD = 11.567502324539603 29\nTHRESHOLD = 15.421624149606197 24\nTHRESHOLD = 19.27574597467279 18\nTHRESHOLD = 23.129867799739383 17\nTHRESHOLD = 26.98398962480598 17\nTHRESHOLD = 30.83811144987257 17\nTHRESHOLD = 34.69223327493916 16\nTHRESHOLD = 38.546355100005755 14\nTHRESHOLD = 42.40047692507235 12\nTHRESHOLD = 46.25459875013894 10\nTHRESHOLD = 50.10872057520553 10\nTHRESHOLD = 53.96284240027213 10\nTHRESHOLD = 57.81696422533872 9\nTHRESHOLD = 61.671086050405314 9\nTHRESHOLD = 65.5252078754719 9\nTHRESHOLD = 69.3793297005385 9\nTHRESHOLD = 73.23345152560509 9\nTHRESHOLD = 77.08757335067169 9\nTHRESHOLD = 80.94169517573827 9\nTHRESHOLD = 84.79581700080487 9\nTHRESHOLD = 88.64993882587147 8\nTHRESHOLD = 92.50406065093806 7\nTHRESHOLD = 96.35818247600466 6\nTHRESHOLD = 100.21230430107124 6\nTHRESHOLD = 104.06642612613784 6\nTHRESHOLD = 107.92054795120444 6\nTHRESHOLD = 111.77466977627103 6\nTHRESHOLD = 115.62879160133762 6\nTHRESHOLD = 119.48291342640421 6\nTHRESHOLD = 123.33703525147081 6\nTHRESHOLD = 127.19115707653741 6\nTHRESHOLD = 131.045278901604 6\nTHRESHOLD = 134.8994007266706 6\nTHRESHOLD = 138.7535225517372 6\nTHRESHOLD = 142.6076443768038 6\nTHRESHOLD = 146.46176620187038 5\nTHRESHOLD = 150.315888026937 5\nTHRESHOLD = 154.17000985200357 5\nTHRESHOLD = 158.02413167707016 5\nTHRESHOLD = 161.87825350213674 5\nTHRESHOLD = 165.73237532720336 5\nTHRESHOLD = 169.58649715226994 5\nTHRESHOLD = 173.44061897733653 5\nTHRESHOLD = 177.29474080240314 5\nTHRESHOLD = 181.14886262746973 5\nTHRESHOLD = 185.0029844525363 5\nTHRESHOLD = 188.85710627760292 5\nTHRESHOLD = 192.7112281026695 5\nTHRESHOLD = 196.5653499277361 5\nTHRESHOLD = 200.41947175280268 5\nTHRESHOLD = 204.2735935778693 5\nTHRESHOLD = 208.12771540293588 5\nTHRESHOLD = 211.98183722800246 4\nTHRESHOLD = 215.83595905306908 4\nTHRESHOLD = 219.69008087813566 4\nTHRESHOLD = 223.54420270320225 4\nTHRESHOLD = 227.39832452826886 4\nTHRESHOLD = 231.25244635333544 4\nTHRESHOLD = 235.10656817840203 4\nTHRESHOLD = 238.9606900034686 4\nTHRESHOLD = 242.81481182853523 4\nTHRESHOLD = 246.6689336536018 4\nTHRESHOLD = 250.5230554786684 4\nTHRESHOLD = 254.377177303735 4\nTHRESHOLD = 258.23129912880154 4\nTHRESHOLD = 262.08542095386815 4\nTHRESHOLD = 265.93954277893477 4\nTHRESHOLD = 269.7936646040013 4\nTHRESHOLD = 273.64778642906793 4\nTHRESHOLD = 277.50190825413455 4\nTHRESHOLD = 281.3560300792011 4\nTHRESHOLD = 285.2101519042677 4\nTHRESHOLD = 289.06427372933433 4\nTHRESHOLD = 292.9183955544009 4\nTHRESHOLD = 296.7725173794675 4\nTHRESHOLD = 300.6266392045341 4\nTHRESHOLD = 304.48076102960067 4\nTHRESHOLD = 308.3348828546673 4\nTHRESHOLD = 312.1890046797339 4\nTHRESHOLD = 316.04312650480045 4\nTHRESHOLD = 319.89724832986707 4\nTHRESHOLD = 323.7513701549336 4\nTHRESHOLD = 327.60549198000024 4\nTHRESHOLD = 331.45961380506685 4\nTHRESHOLD = 335.3137356301334 3\nTHRESHOLD = 339.1678574552 3\nTHRESHOLD = 343.02197928026663 3\nTHRESHOLD = 346.8761011053332 2\nTHRESHOLD = 350.7302229303998 2\nTHRESHOLD = 354.5843447554664 2\nTHRESHOLD = 358.438466580533 2\nTHRESHOLD = 362.2925884055996 2\nTHRESHOLD = 366.1467102306662 2\nTHRESHOLD = 370.00083205573276 2\nTHRESHOLD = 373.85495388079937 2\nTHRESHOLD = 377.709075705866 2\nTHRESHOLD = 381.56319753093254 2\nTHRESHOLD = 385.41731935599915 2\nTHRESHOLD = 389.2714411810657 2\nTHRESHOLD = 393.1255630061323 2\nTHRESHOLD = 396.97968483119894 2\nTHRESHOLD = 400.8338066562655 2\nTHRESHOLD = 404.6879284813321 2\nTHRESHOLD = 408.5420503063987 2\nTHRESHOLD = 412.3961721314653 2\nTHRESHOLD = 416.2502939565319 2\nTHRESHOLD = 420.1044157815985 2\nTHRESHOLD = 423.95853760666506 2\nTHRESHOLD = 427.8126594317317 2\nTHRESHOLD = 431.6667812567983 2\nTHRESHOLD = 435.52090308186484 2\nTHRESHOLD = 439.37502490693146 2\nTHRESHOLD = 443.22914673199807 2\nTHRESHOLD = 447.0832685570646 2\nTHRESHOLD = 450.93739038213124 2\nTHRESHOLD = 454.79151220719785 2\nTHRESHOLD = 458.6456340322644 2\nTHRESHOLD = 462.499755857331 2\nTHRESHOLD = 466.3538776823976 2\nTHRESHOLD = 470.2079995074642 2\nTHRESHOLD = 474.0621213325308 2\nTHRESHOLD = 477.91624315759736 2\nTHRESHOLD = 481.770364982664 2\nTHRESHOLD = 485.6244868077306 2\nTHRESHOLD = 489.47860863279715 2\nTHRESHOLD = 493.33273045786376 2\nTHRESHOLD = 497.1868522829304 1\nTHRESHOLD = 501.04097410799693 1\nTHRESHOLD = 504.89509593306354 1\nTHRESHOLD = 508.74921775813016 1\nTHRESHOLD = 512.6033395831968 1\nTHRESHOLD = 516.4574614082633 1\nTHRESHOLD = 520.3115832333299 1\nTHRESHOLD = 524.1657050583965 1\nTHRESHOLD = 528.0198268834631 1\nTHRESHOLD = 531.8739487085297 1\nTHRESHOLD = 535.7280705335963 1\nTHRESHOLD = 539.5821923586628 1\nTHRESHOLD = 543.4363141837295 1\nTHRESHOLD = 547.2904360087961 1\nTHRESHOLD = 551.1445578338627 1\nTHRESHOLD = 554.9986796589293 1\nTHRESHOLD = 558.8528014839959 1\nTHRESHOLD = 562.7069233090624 1\nTHRESHOLD = 566.561045134129 1\nTHRESHOLD = 570.4151669591956 1\nTHRESHOLD = 574.2692887842622 1\nTHRESHOLD = 578.1234106093289 1\nTHRESHOLD = 581.9775324343954 1\nTHRESHOLD = 585.831654259462 1\nTHRESHOLD = 589.6857760845286 1\nTHRESHOLD = 593.5398979095952 1\nTHRESHOLD = 597.3940197346618 1\nTHRESHOLD = 601.2481415597284 1\nTHRESHOLD = 605.1022633847949 1\nTHRESHOLD = 608.9563852098615 1\nTHRESHOLD = 612.8105070349282 1\nTHRESHOLD = 616.6646288599948 1\nTHRESHOLD = 620.5187506850614 1\nTHRESHOLD = 624.372872510128 1\nTHRESHOLD = 628.2269943351945 1\nTHRESHOLD = 632.0811161602611 1\nTHRESHOLD = 635.9352379853277 1\nTHRESHOLD = 639.7893598103943 1\nTHRESHOLD = 643.643481635461 1\nTHRESHOLD = 647.4976034605274 1\nTHRESHOLD = 651.3517252855941 1\nTHRESHOLD = 655.2058471106607 1\nTHRESHOLD = 659.0599689357273 1\nTHRESHOLD = 662.9140907607939 1\nTHRESHOLD = 666.7682125858605 1\nTHRESHOLD = 670.622334410927 1\nTHRESHOLD = 674.4764562359936 1\nTHRESHOLD = 678.3305780610602 1\nTHRESHOLD = 682.1846998861269 1\nTHRESHOLD = 686.0388217111935 1\nTHRESHOLD = 689.8929435362601 1\nTHRESHOLD = 693.7470653613266 1\nTHRESHOLD = 697.6011871863932 1\nTHRESHOLD = 701.4553090114598 1\nTHRESHOLD = 705.3094308365264 1\nTHRESHOLD = 709.163552661593 1\nTHRESHOLD = 713.0176744866595 1\nTHRESHOLD = 716.8717963117261 1\nTHRESHOLD = 720.7259181367928 1\nTHRESHOLD = 724.5800399618594 1\nTHRESHOLD = 728.434161786926 1\nTHRESHOLD = 732.2882836119926 1\nTHRESHOLD = 736.1424054370591 1\nTHRESHOLD = 739.9965272621257 1\nTHRESHOLD = 743.8506490871923 1\nTHRESHOLD = 747.7047709122589 1\nTHRESHOLD = 751.5588927373256 1\nTHRESHOLD = 755.4130145623922 1\nTHRESHOLD = 759.2671363874587 1\nTHRESHOLD = 763.1212582125253 1\nTHRESHOLD = 766.9753800375919 1\nTHRESHOLD = 770.8295018626585 1\nTHRESHOLD = 774.6836236877251 1\nTHRESHOLD = 778.5377455127916 1\nTHRESHOLD = 782.3918673378582 1\nTHRESHOLD = 786.2459891629248 1\nTHRESHOLD = 790.1001109879915 1\nTHRESHOLD = 793.9542328130581 1\nTHRESHOLD = 797.8083546381247 1\nTHRESHOLD = 801.6624764631912 1\nTHRESHOLD = 805.5165982882578 1\nTHRESHOLD = 809.3707201133244 1\nTHRESHOLD = 813.224841938391 1\nTHRESHOLD = 817.0789637634576 1\nTHRESHOLD = 820.9330855885242 1\nTHRESHOLD = 824.7872074135907 1\nTHRESHOLD = 828.6413292386574 1\nTHRESHOLD = 832.495451063724 1\nTHRESHOLD = 836.3495728887906 1\nTHRESHOLD = 840.2036947138572 1\nTHRESHOLD = 844.0578165389238 1\nTHRESHOLD = 847.9119383639903 1\nTHRESHOLD = 851.7660601890569 1\nTHRESHOLD = 855.6201820141235 1\nTHRESHOLD = 859.4743038391902 1\nTHRESHOLD = 863.3284256642568 1\nTHRESHOLD = 867.1825474893233 1\nTHRESHOLD = 871.0366693143899 1\nTHRESHOLD = 874.8907911394565 1\nTHRESHOLD = 878.7449129645231 1\nTHRESHOLD = 882.5990347895897 1\nTHRESHOLD = 886.4531566146563 1\nTHRESHOLD = 890.3072784397228 1\nTHRESHOLD = 894.1614002647894 1\nTHRESHOLD = 898.0155220898561 1\nTHRESHOLD = 901.8696439149227 1\nTHRESHOLD = 905.7237657399893 1\nTHRESHOLD = 909.5778875650559 1\nTHRESHOLD = 913.4320093901224 1\nTHRESHOLD = 917.286131215189 1\nTHRESHOLD = 921.1402530402556 1\nTHRESHOLD = 924.9943748653222 1\nTHRESHOLD = 928.8484966903889 1\nTHRESHOLD = 932.7026185154554 1\nTHRESHOLD = 936.556740340522 1\nTHRESHOLD = 940.4108621655886 1\nTHRESHOLD = 944.2649839906552 1\nTHRESHOLD = 948.1191058157218 1\nTHRESHOLD = 951.9732276407884 1\nTHRESHOLD = 955.8273494658549 1\nTHRESHOLD = 959.6814712909215 1\nTHRESHOLD = 963.5355931159881 1\nTHRESHOLD = 967.3897149410548 1\nTHRESHOLD = 971.2438367661214 1\nTHRESHOLD = 975.097958591188 1\nTHRESHOLD = 978.9520804162545 1\nTHRESHOLD = 982.8062022413211 1\nTHRESHOLD = 986.6603240663877 1\nTHRESHOLD = 990.5144458914543 1\nTHRESHOLD = 994.3685677165209 1\nTHRESHOLD = 998.2226895415874 1\nTHRESHOLD = 1002.076811366654 1\nTHRESHOLD = 1005.9309331917207 1\nTHRESHOLD = 1009.7850550167873 1\nTHRESHOLD = 1013.6391768418539 1\nTHRESHOLD = 1017.4932986669205 1\nTHRESHOLD = 1021.347420491987 1\nTHRESHOLD = 1025.2015423170537 1\nTHRESHOLD = 1029.0556641421203 1\nTHRESHOLD = 1032.9097859671867 1\nTHRESHOLD = 1036.7639077922533 1\nTHRESHOLD = 1040.61802961732 1\nTHRESHOLD = 1044.4721514423866 1\nTHRESHOLD = 1048.3262732674532 1\nTHRESHOLD = 1052.1803950925198 1\nTHRESHOLD = 1056.0345169175864 1\nTHRESHOLD = 1059.888638742653 1\nTHRESHOLD = 1063.7427605677196 1\nTHRESHOLD = 1067.5968823927863 1\nTHRESHOLD = 1071.4510042178529 1\nTHRESHOLD = 1075.3051260429193 1\nTHRESHOLD = 1079.1592478679859 1\nTHRESHOLD = 1083.0133696930525 1\nTHRESHOLD = 1086.867491518119 1\nTHRESHOLD = 1090.7216133431857 1\nTHRESHOLD = 1094.5757351682523 1\nTHRESHOLD = 1098.429856993319 1\nTHRESHOLD = 1102.2839788183855 1\nTHRESHOLD = 1106.1381006434522 1\nTHRESHOLD = 1109.9922224685188 1\nTHRESHOLD = 1113.8463442935854 1\nTHRESHOLD = 1117.700466118652 1\nTHRESHOLD = 1121.5545879437184 1\nTHRESHOLD = 1125.408709768785 1\nTHRESHOLD = 1129.2628315938516 1\nTHRESHOLD = 1133.1169534189182 1\nTHRESHOLD = 1136.9710752439848 1\nTHRESHOLD = 1140.8251970690515 1\nTHRESHOLD = 1144.679318894118 1\n\n\n\nmax_acc\n\n91.45299145299145\n\n\nWe can see the result from the variance threshold is better to the partial grid search. So we choose this as the final result.\n\nplot_results()\n\n\n\n\n\n\n\n\n\n\n\nThis graph speaks the same thing as the first graph of the last group. But notice the features number are dramtically downed. Which means a lot of features were at a very low variance or extremely high which got cutted off when cutting the edge value.\nThe second one shows the time cost with differnt amount of features, again, it was quite random when the feature number was small, but after 30 features, the time cost starts to goes relatively stable.\nThis shows when the number of feature goes up, the difference between training and testing accuracy goes bigger and bigger. Combine with the result from the first graph, we can see as the feature goes up the modle is becoming overfitting."
  },
  {
    "objectID": "nb_text.html#final-result",
    "href": "nb_text.html#final-result",
    "title": "Naive Bayes on Text Data",
    "section": "Final Result",
    "text": "Final Result\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef generate_final_result(X, Y):\n\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # class_prior=[prior_0,prior_1]\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n\n    # TRAIN MODEL \n    model.fit(x_train,y_train)\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n\n    f1 = f1_score(y_test, yp_test)\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n\n    return acc_train,acc_test, f1, yp_test, y_test\n\n\nacc_train,acc_test, f1, yp_test, y_test = generate_final_result(x_best, y)\n\n\nacc_train\n\n92.67241379310344\n\n\n\nacc_test\n\n91.45299145299145\n\n\nWe can see after feature engineering, the overfitting problem was solved. The accuracy is basically the same during training and testing.\n\nf1\n\n0.9166666666666667\n\n\nA good F1 score, it is well above 0.75\n\nconf_matrix = metrics.confusion_matrix(y_test, yp_test, labels=[0, 1])\n\nplt.title(\"Confusion matrix\")\naxis = sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\naxis.set_xticklabels(['2010-2014', '2015-2019'])\naxis.set_yticklabels(['2010-2014', '2015-2019'])\naxis.set(xlabel=\"Predicted\", ylabel=\"True\")\nplt.show()"
  },
  {
    "objectID": "nb_text.html#discussion",
    "href": "nb_text.html#discussion",
    "title": "Naive Bayes on Text Data",
    "section": "Discussion",
    "text": "Discussion\n\nThe modle was trained using the sklern inbulit MultinomialNB function. Using the fit method to train on the training dataset and predict method to test on the test set.\nThe model has a both a good accuracy and a good F1. From the confusion matrix, we can see the the balance between the precision and recall is good as well. There are no sign of overfitting or underfitting.\nThe sign of overfitting and underfitting were discussed in the other use case of naive bayes classifier which was on the record data. For this, I will just discuss the conclusion, which is there are no sign of overfitting or underfitting in this model. The bias and variance are both low.\nThe project finds that predicting the valence mode for a song uisng it’s lyric is highly doable by simple naive bayes. If there is a need for a report, this should be documented using like a R markdown or Jupyter notebook so the result is reproduceable. What should be reported include how the modle was trained, what is the dataset, the result and the limitation."
  },
  {
    "objectID": "nb_text.html#conclusion",
    "href": "nb_text.html#conclusion",
    "title": "Naive Bayes on Text Data",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, using a naive bayes on the lyrics data is a good idea. The model has a good accuarcy and a good F1 score, and from the confusion matrix we can see it perfome eqaully good on both classes. As a result, I think this model can be really useful in the real practice."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Tianze Yang, currently studying in Georgetown University’s DSAN Master program. I graduate from both the University of South Australia with a Bachelor’s degree in Software Engineering (Honors) and Xi’an University of Architecture and Technology in Computer Science.\nFor my internship experiences, I worked for ZhiguangHailian Big Data Technology Co., LTD, and Zhihu Inc. During these internships, I improved my skills in data processing, software development, and machine learning in real world practice. At ZhiguangHailian, I developed a Python algorithm to determine GPS coordinates from textual geographic locations and determine which district it belongs to. At Zhihu Inc., I was heavily involved into data cleaning, especially for creating training and testing set for their Large Language Model. I also had a chance to use that model create application to detect adult contents on their website.\nI also have some academic projects. I’ve worked on projects like the SA Opportunity Atlas under the guidance of Justin Anderson from MIT for my bachelor graduation. Additionally, I researched the application of AI in medical treatment under the mentorship of Dianbo Liu from Harvard University. These projects allowed me to practice a wide range of both frontend and machine learning skills. In terms of programming skills, I am proficient in programming languages Python, Java and Swift. I have experience with web development tools such as .NET, C#, Bootstrap, and Angular JS. My expertise also extends to data analysis with tools like SQL and R, and I am familiar with distributed systems like Apache Hadoop."
  },
  {
    "objectID": "about.html#self-introduction",
    "href": "about.html#self-introduction",
    "title": "About Me",
    "section": "",
    "text": "My name is Tianze Yang, currently studying in Georgetown University’s DSAN Master program. I graduate from both the University of South Australia with a Bachelor’s degree in Software Engineering (Honors) and Xi’an University of Architecture and Technology in Computer Science.\nFor my internship experiences, I worked for ZhiguangHailian Big Data Technology Co., LTD, and Zhihu Inc. During these internships, I improved my skills in data processing, software development, and machine learning in real world practice. At ZhiguangHailian, I developed a Python algorithm to determine GPS coordinates from textual geographic locations and determine which district it belongs to. At Zhihu Inc., I was heavily involved into data cleaning, especially for creating training and testing set for their Large Language Model. I also had a chance to use that model create application to detect adult contents on their website.\nI also have some academic projects. I’ve worked on projects like the SA Opportunity Atlas under the guidance of Justin Anderson from MIT for my bachelor graduation. Additionally, I researched the application of AI in medical treatment under the mentorship of Dianbo Liu from Harvard University. These projects allowed me to practice a wide range of both frontend and machine learning skills. In terms of programming skills, I am proficient in programming languages Python, Java and Swift. I have experience with web development tools such as .NET, C#, Bootstrap, and Angular JS. My expertise also extends to data analysis with tools like SQL and R, and I am familiar with distributed systems like Apache Hadoop."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n2018 - 2020: Xi’an University of Architecture and Technology, Xi’an, China Degree: Bachelor of Computer Science\n2020 - 2022: University of South Australia, Adelaide, SA Degree: Bachelor of Software Engineering (Honors)\n2023 - now: Georgetonw University, Washington, DC Program: Master of Science in Data Science and Analytics"
  },
  {
    "objectID": "about.html#academic-interests",
    "href": "about.html#academic-interests",
    "title": "About Me",
    "section": "Academic Interests",
    "text": "Academic Interests\n\nComputer Science and related diciplines\nMachine Learning\nData Scinece\nAlso want to learn more about biology if I could in the future."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Protofilio Project",
    "section": "",
    "text": "Welcome to my Protofilio Project website!"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code",
    "section": "",
    "text": "Here is a code repository of this project in GitHub: Project Repository"
  },
  {
    "objectID": "code.html#project-repository",
    "href": "code.html#project-repository",
    "title": "Code",
    "section": "",
    "text": "Here is a code repository of this project in GitHub: Project Repository"
  },
  {
    "objectID": "data_exploration.html",
    "href": "data_exploration.html",
    "title": "Data Exploration",
    "section": "",
    "text": "In this section, we will focus on exploring the data and understanding the data. For each dataset, we will try to find interesting trait or correlation between them or across dataset in order to answer the questions we proposed and refine our hypothesis. We will use the following tools to help us in this process: Pandas and Seaborn.\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# load the data\ndf_spotify_track = pd.read_csv('../data/01-modified-data/spotify_current_all.csv')\n\n\nprint(df_spotify_track.head())\n\n   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n0         0.864   0.556    2    -7.683     0       0.1940         0.255   \n1         0.802   0.832   11    -4.107     1       0.0434         0.311   \n2         0.552   0.702    9    -5.707     1       0.1570         0.117   \n3         0.841   0.738    7    -7.455     0       0.3070         0.520   \n4         0.628   0.523   11    -8.307     0       0.0946         0.701   \n\n   instrumentalness  liveness  valence    tempo                track_id  \\\n0          0.000004    0.1120    0.726   99.974  56y1jOTK0XSvJzVv9vHQBK   \n1          0.000000    0.0815    0.890  124.997  7x9aauaA9cu6tyfpHnqDLo   \n2          0.000021    0.1050    0.564  169.994  1BxfuPKGuaTgP7aM0Bbdwr   \n3          0.000000    0.0892    0.484  169.918  5RqSsdzTNPX1uzkmlHCFvK   \n4          0.002740    0.2190    0.416  169.982  5mjYQaktjmjcMKcUIcqz4s   \n\n               artist_ids                           track_name          name  \\\n0  5cj0lLjcoR7YOSnhnX0Po5                   Paint The Town Red      Doja Cat   \n1  6HaGTQPmzraVmaVxvz6EUc  Seven (feat. Latto) (Explicit Ver.)     Jung Kook   \n2  06HL4z0CvFAxyc27GXpf02                         Cruel Summer  Taylor Swift   \n3  790FomKkXshlbRYZFtlgla                                QLONA       KAROL G   \n4  7uMDnSZyUYNBPLhPMNuaM2                            Strangers   Kenya Grace   \n\n  broad_genre  \n0         pop  \n1         pop  \n2         pop  \n3       latin  \n4      others  \n\n\nHere is a statisiical summary of the dataset for popular songs.\n\nprint(df_spotify_track.describe())\n\n       danceability      energy         key    loudness        mode  \\\ncount    187.000000  187.000000  187.000000  187.000000  187.000000   \nmean       0.652583    0.640706    5.085561   -6.201134    0.647059   \nstd        0.145082    0.164622    3.582087    2.139002    0.479168   \nmin        0.351000    0.091100    0.000000  -17.665000    0.000000   \n25%        0.541000    0.535500    1.000000   -7.515000    0.000000   \n50%        0.647000    0.669000    5.000000   -5.745000    1.000000   \n75%        0.767000    0.762500    8.000000   -4.635500    1.000000   \nmax        0.971000    0.989000   11.000000   -2.278000    1.000000   \n\n       speechiness  acousticness  instrumentalness    liveness     valence  \\\ncount   187.000000    187.000000        187.000000  187.000000  187.000000   \nmean      0.085687      0.242592          0.014381    0.168414    0.507722   \nstd       0.083841      0.247013          0.076317    0.122816    0.244540   \nmin       0.025400      0.000390          0.000000    0.029700    0.034800   \n25%       0.036750      0.048200          0.000000    0.097050    0.318500   \n50%       0.053800      0.145000          0.000000    0.120000    0.488000   \n75%       0.088300      0.405000          0.000027    0.186000    0.696500   \nmax       0.397000      0.959000          0.629000    0.923000    0.964000   \n\n            tempo  \ncount  187.000000  \nmean   122.745358  \nstd     29.703599  \nmin     66.041000  \n25%     98.516500  \n50%    122.811000  \n75%    143.019500  \nmax    203.759000  \n\n\n\n\n\nsns.countplot(x='broad_genre', data=df_spotify_track)\nplt.xticks(rotation=90)\n\nplt.show()\n\n\n\n\nFrom this plot, it’s evident that pop music dominates the current top songs, followed by hip hop as the second most popular genre. Notably, the third most prevalent category is ‘others,’ which comprises songs that don’t fit neatly into established genres. This might hint at the growing trend of genreless music. However, a comparison with older songs is necessary to determine if this is a recent phenomenon.\n\n\n\n\n\n# load the data\ndf_spotify_history = pd.read_csv('../data/01-modified-data/top50MusicFrom2010-2019_cleaned.csv')\n\n\ndf_spotify_history.head()\n\n\n\n\n\n\n\n\ntitle\nartist\ngenre\nyear\nbeat\nenergy\ndanceability\nloudness\nvalence\nduration\nacousticness\nspeechiness\npopularity\nyear_group\n\n\n\n\n0\nHey, Soul Sister\nTrain\npop\n2010\n97\n89\n67\n-4\n80\n217\n19\n4\n83\n2010-2014\n\n\n1\nLove The Way You Lie\nEminem\nhip_hop_rap\n2010\n87\n93\n75\n-5\n64\n263\n24\n23\n82\n2010-2014\n\n\n2\nTiK ToK\nKesha\npop\n2010\n120\n84\n76\n-3\n71\n200\n10\n14\n80\n2010-2014\n\n\n3\nBad Romance\nLady Gaga\npop\n2010\n119\n92\n70\n-4\n71\n295\n0\n4\n79\n2010-2014\n\n\n4\nJust the Way You Are\nBruno Mars\npop\n2010\n109\n84\n64\n-5\n43\n221\n2\n4\n78\n2010-2014\n\n\n\n\n\n\n\nStatistics summary\n\ndf_spotify_history.describe()\n\n\n\n\n\n\n\n\nyear\nbeat\nenergy\ndanceability\nloudness\nvalence\nduration\nacousticness\nspeechiness\npopularity\n\n\n\n\ncount\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n\n\nmean\n2014.592040\n118.545605\n70.504146\n64.379768\n-5.578773\n52.225539\n224.674959\n14.326700\n8.358209\n66.520730\n\n\nstd\n2.607057\n24.795358\n16.310664\n13.378718\n2.798020\n22.513020\n34.130059\n20.766165\n7.483162\n14.517746\n\n\nmin\n2010.000000\n0.000000\n0.000000\n0.000000\n-60.000000\n0.000000\n134.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n2013.000000\n100.000000\n61.000000\n57.000000\n-6.000000\n35.000000\n202.000000\n2.000000\n4.000000\n60.000000\n\n\n50%\n2015.000000\n120.000000\n74.000000\n66.000000\n-5.000000\n52.000000\n221.000000\n6.000000\n5.000000\n69.000000\n\n\n75%\n2017.000000\n129.000000\n82.000000\n73.000000\n-4.000000\n69.000000\n239.500000\n17.000000\n9.000000\n76.000000\n\n\nmax\n2019.000000\n206.000000\n98.000000\n97.000000\n-2.000000\n98.000000\n424.000000\n99.000000\n48.000000\n99.000000\n\n\n\n\n\n\n\nWe can observe from the basic statistic that most of the statistic are quite similar to the ones in the currently hot songs.\n\ndf_spotify_history.columns\n\nIndex(['title', 'artist', 'genre', 'year', 'beat', 'energy', 'danceability',\n       'loudness', 'valence', 'duration', 'acousticness', 'speechiness',\n       'popularity', 'year_group'],\n      dtype='object')\n\n\n\ndf_histroy_melted  = df_spotify_history.melt(id_vars=['title', 'artist', 'year', 'beat', 'energy', 'danceability',\n       'loudness', 'valence', 'duration', 'acousticness', 'speechiness',\n       'popularity', 'year_group'], var_name='feature', value_name='value')\n\n\nsns.countplot(x='value', data=df_histroy_melted, hue='value')\nplt.xticks(rotation=90)\n\nplt.show()\n\n\n\n\nFrom this plot, it’s evident that the past decade witnessed an overwhelming dominance of pop music, comprising nearly all of the top songs. Electronic dance music emerged as the second most successful genre, with hip hop not even ranking third in popularity. There appears to have been some significant shifts in musical trends over the past decade. To highlight these changes more clearly, we should create another graph comparing the genre proportions from the last decade with those in the current top songs.\n\n# construst the dataset\ndf1 = df_spotify_history['genre'].value_counts() / df_spotify_history['genre'].value_counts().sum()\n\ndf2 = df_spotify_track['broad_genre'].value_counts() / df_spotify_track['broad_genre'].value_counts().sum()\n\n\ndf1 = pd.DataFrame(df1)\n# df1.set_index('genre').T\n\n\ndf1\ndf1.reset_index(inplace=True)\ndf1['type'] = 'history'\n\n\ndf2 = pd.DataFrame(df2)\ndf2.reset_index(inplace=True)\ndf2['type'] = 'current'\ndf2.rename(columns={'broad_genre':'genre'}, inplace=True)\n\n\ndf_genre_compare = pd.concat([df1, df2], axis=0)\n\n\nsns.barplot(x='genre', y='count', data=df_genre_compare, hue='type')\nplt.xticks(rotation=90)\nplt.ylabel('Percentage')\nplt.show()\n\n\n\n\nThis plot contrasts the proportion of each genre in the current top songs with those from the past decade. Notably, pop music’s share has seen a substantial decline, while other genres, particularly hip hop (now the second most prevalent genre in current top songs), have witnessed a surge. A significant observation is the minimal percentage of the ‘others’ category in past top songs, which now claims a notable share. This might be attributed to the emergence of hybrid genres like ‘gauze pop’ and ‘escape room’ that don’t fit conventional classifications and have only gained recognition recently. Latin music has also experienced a resurgence. In summary, there’s been a shift in genre distribution over the years, but a more detailed time-based analysis is needed to capture the full evolution.\n\ndf_genre_change = df_spotify_history.copy()\ndf_genre_change['count'] = 0\nfor i in range(len(df_genre_change)):\n    df_genre_change['count'][i] = len(df_genre_change[df_genre_change['year'] == df_genre_change['year'][i]])\n\n\ndf_genre_change['propotion'] = 1 / df_genre_change['count']\n\n\nsns.set_theme()\n\n# Create line plots for each genre or attribute\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=df_genre_change[df_genre_change['genre'] == 'pop'], x='year', y='propotion', label='Pop Proportion', estimator='sum')\nsns.lineplot(data=df_genre_change[df_genre_change['genre'] == 'hip_hop_rap'], x='year', y='propotion', label='Hip Hop Proportion', estimator='sum')\nsns.lineplot(data=df_genre_change[df_genre_change['genre'] == 'others'], x='year', y='propotion', label='Others Proportion', estimator='sum')\n# ... Add other attributes as needed\n\n# sns.lineplot(data=df_spotify_history, x='year', y='energy', label='Energy')\n\n# Add title, labels, and legend\nplt.title('Distribution of Musical Attributes Over the Years')\nplt.xlabel('Year')\nplt.ylabel('Value')\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\nIn the presented plot, we observe that the proportion of pop music within top songs each year doesn’t show a consistent decline but fluctuates over time. Nonetheless, the overarching trend suggests a decrease in the popularity of pop music. Hip hop music experienced a marked drop starting in 2010, stabilizing for several years thereafter. From 2015 onwards, hip hop’s proportion began to rise again. Furthermore, the “others” category, representing emerging genres not easily classified into traditional categories, began to see an uptick starting in 2018.\n\n\n\nTo do so, we will need to see the distribution of multipy musical features in the top songs over time.\n\nsns.set_theme()\n\n# Create line plots for each genre or attribute\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=df_spotify_history, x='year', y='beat', label='Beat', estimator='median')\nsns.lineplot(data=df_spotify_history, x='year', y='energy', label='Energy')\nsns.lineplot(data=df_spotify_history, x='year', y='danceability', label='Danceability')\n# ... Add other attributes as needed\n\n# sns.lineplot(data=df_spotify_history, x='year', y='energy', label='Energy')\n\n# Add title, labels, and legend\nplt.title('Distribution of Musical Attributes Over the Years')\nplt.xlabel('Year')\nplt.ylabel('Value')\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\nThe preceding graph illustrates the evolution of beats, danceability, and energy distributions over a decade. The line represents the median, while the shaded area encompasses the 25th to 75th percentiles. Contrary to expectations, the distributions of these features have not become more homogeneous over time. In fact, their distributions have remained relatively consistent. Notably, the range of beats has widened compared to earlier years. Regarding median values, there is a noticeable decline in both beats and energy over the years, while danceability has seen an upward trend.\n\n\n\n\n# load the data\ndf_tik_tok = pd.read_csv('../data/01-modified-data/tik_tok_cleaned.csv')\n\n\ndf_tik_tok.head()\n\n\n\n\n\n\n\n\ntrack_name\nartist\ntrack_pop\ndanceability\nenergy\nloudness\nmode\nkey\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\nduration_ms\n\n\n\n\n0\nRunning Up That Hill (A Deal With God)\nKate Bush\n95\n0.629\n0.547\n-13.123\n0\n10\n0.0550\n0.7200\n0.003140\n0.0604\n0.197\n108.375\n4\n298933\n\n\n1\nAs It Was\nHarry Styles\n96\n0.520\n0.731\n-5.338\n0\n6\n0.0557\n0.3420\n0.001010\n0.3110\n0.662\n173.930\n4\n167303\n\n\n2\nSunroof\nNicky Youre\n44\n0.768\n0.716\n-5.110\n1\n10\n0.0404\n0.3500\n0.000000\n0.1500\n0.841\n131.430\n4\n163026\n\n\n3\nHeat Waves\nGlass Animals\n89\n0.761\n0.525\n-6.900\n1\n11\n0.0944\n0.4400\n0.000007\n0.0921\n0.531\n80.870\n4\n238805\n\n\n4\nAbout Damn Time\nLizzo\n92\n0.836\n0.743\n-6.305\n0\n10\n0.0656\n0.0995\n0.000000\n0.3350\n0.722\n108.966\n4\n191822\n\n\n\n\n\n\n\nStatistics summary\n\ndf_tik_tok.describe\n\n&lt;bound method NDFrame.describe of                                  track_name           artist  track_pop  \\\n0    Running Up That Hill (A Deal With God)        Kate Bush         95   \n1                                 As It Was     Harry Styles         96   \n2                                   Sunroof      Nicky Youre         44   \n3                                Heat Waves    Glass Animals         89   \n4                           About Damn Time            Lizzo         92   \n..                                      ...              ...        ...   \n258              The Less I Know The Better      Tame Impala         84   \n259                              Dandelions          Ruth B.         90   \n260           Jimmy Cooks (feat. 21 Savage)            Drake         92   \n261                            Good Looking  Suki Waterhouse         80   \n262                                 INFERNO        Sub Urban         71   \n\n     danceability  energy  loudness  mode  key  speechiness  acousticness  \\\n0           0.629   0.547   -13.123     0   10       0.0550      0.720000   \n1           0.520   0.731    -5.338     0    6       0.0557      0.342000   \n2           0.768   0.716    -5.110     1   10       0.0404      0.350000   \n3           0.761   0.525    -6.900     1   11       0.0944      0.440000   \n4           0.836   0.743    -6.305     0   10       0.0656      0.099500   \n..            ...     ...       ...   ...  ...          ...           ...   \n258         0.640   0.740    -4.083     1    4       0.0284      0.011500   \n259         0.609   0.692    -2.958     1    1       0.0259      0.015700   \n260         0.529   0.673    -4.711     1    0       0.1750      0.000307   \n261         0.377   0.558    -9.076     1    4       0.0299      0.078900   \n262         0.820   0.611    -5.020     0    9       0.1220      0.076600   \n\n     instrumentalness  liveness  valence    tempo  time_signature  duration_ms  \n0            0.003140    0.0604    0.197  108.375               4       298933  \n1            0.001010    0.3110    0.662  173.930               4       167303  \n2            0.000000    0.1500    0.841  131.430               4       163026  \n3            0.000007    0.0921    0.531   80.870               4       238805  \n4            0.000000    0.3350    0.722  108.966               4       191822  \n..                ...       ...      ...      ...             ...          ...  \n258          0.006780    0.1670    0.785  116.879               4       216320  \n259          0.000000    0.0864    0.454  116.959               3       233720  \n260          0.000002    0.0930    0.366  165.921               4       218365  \n261          0.000342    0.1250    0.267  149.971               3       214800  \n262          0.000025    0.0684    0.637  127.883               4       133134  \n\n[263 rows x 16 columns]&gt;\n\n\n\ndf_spotify_history.columns\n\nIndex(['title', 'artist', 'genre', 'year', 'beat', 'energy', 'danceability',\n       'loudness', 'valence', 'duration', 'acousticness', 'speechiness',\n       'popularity', 'year_group'],\n      dtype='object')\n\n\n\n# construst the dataset\ndf1 = df_spotify_track.copy()\ndf1['type'] = 'spotify'\ndf1.rename(columns={'broad_genre':'genre'}, inplace=True)\ndf2 = df_tik_tok.copy()\ndf2['type'] = 'tik_tok'\n\ndf_tik_spotify = pd.concat([df1, df2], axis=0)\n\n\ndf_tik_spotify.head()\n\n\n\n\n\n\n\n\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\n...\ntrack_id\nartist_ids\ntrack_name\nname\ngenre\ntype\nartist\ntrack_pop\ntime_signature\nduration_ms\n\n\n\n\n0\n0.864\n0.556\n2\n-7.683\n0\n0.1940\n0.255\n0.000004\n0.1120\n0.726\n...\n56y1jOTK0XSvJzVv9vHQBK\n5cj0lLjcoR7YOSnhnX0Po5\nPaint The Town Red\nDoja Cat\npop\nspotify\nNaN\nNaN\nNaN\nNaN\n\n\n1\n0.802\n0.832\n11\n-4.107\n1\n0.0434\n0.311\n0.000000\n0.0815\n0.890\n...\n7x9aauaA9cu6tyfpHnqDLo\n6HaGTQPmzraVmaVxvz6EUc\nSeven (feat. Latto) (Explicit Ver.)\nJung Kook\npop\nspotify\nNaN\nNaN\nNaN\nNaN\n\n\n2\n0.552\n0.702\n9\n-5.707\n1\n0.1570\n0.117\n0.000021\n0.1050\n0.564\n...\n1BxfuPKGuaTgP7aM0Bbdwr\n06HL4z0CvFAxyc27GXpf02\nCruel Summer\nTaylor Swift\npop\nspotify\nNaN\nNaN\nNaN\nNaN\n\n\n3\n0.841\n0.738\n7\n-7.455\n0\n0.3070\n0.520\n0.000000\n0.0892\n0.484\n...\n5RqSsdzTNPX1uzkmlHCFvK\n790FomKkXshlbRYZFtlgla\nQLONA\nKAROL G\nlatin\nspotify\nNaN\nNaN\nNaN\nNaN\n\n\n4\n0.628\n0.523\n11\n-8.307\n0\n0.0946\n0.701\n0.002740\n0.2190\n0.416\n...\n5mjYQaktjmjcMKcUIcqz4s\n7uMDnSZyUYNBPLhPMNuaM2\nStrangers\nKenya Grace\nothers\nspotify\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 21 columns\n\n\n\n\nsns.scatterplot(data=df_tik_spotify, x='danceability', y='energy', hue='type')\n\n&lt;Axes: xlabel='danceability', ylabel='energy'&gt;\n\n\n\n\n\nThis scatter plot depicts the relationship between song popularity and danceability for tracks popular on TikTok versus those favored on streaming services. Songs trending on TikTok tend to cluster around higher energy and danceability levels. This may be attributed to TikTok being a short-video platform, where more energetic and danceable songs are likely to thrive. In contrast, songs popular on streaming services appear more dispersed without an evident pattern. Notably, there are outliers: highly popular songs that aren’t particularly danceable, meriting further analysis.\n\n\n\n\n# Load the data\ndf_tracks = pd.read_csv('../data/00-raw-data/tracks.csv')\n\n\ncorr_1 = df_tracks.corr(method='pearson',numeric_only=True)  \n\nmask = np.triu(np.ones_like(corr_1, dtype=bool)) \nf, ax = plt.subplots(figsize=(11, 9)) \ncmap = sns.diverging_palette(230, 20, as_cmap=True) \n\nsns.heatmap(corr_1, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\nplt.show()\n\n\n\n\nFrom the heatmap, we can discern that most variables do not exhibit strong correlations with each other. Importantly, none of the values showcase a high correlation with popularity. The features most positively correlated with popularity are loudness and energy, with both having nearly identical correlation values. On the other end, acousticness is the most negatively correlated feature, followed closely by instrumentalness. Among all the features, energy and loudness are the most closely related, which makes sense as loudness often corresponds with a song’s energy. Based on these observations, we can delve deeper into the relationship between popularity, energy, and loudness. Additionally, both danceability and loudness have a positive correlation with a song’s valence.\n\ndf_tracks['release_date'] = pd.to_datetime(df_tracks['release_date'], errors='coerce')\n\n\ndf_2010 = df_tracks[df_tracks['release_date'].dt.year &gt;= 2010]\n\n\ndf_2010['type'] = 'Others'\n\n/var/folders/s0/4jmlz4bn2jv0712y1pv_xnbh0000gn/T/ipykernel_2997/361325449.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_2010['type'] = 'Others'\n\n\n\ndf_energy = df_2010[['energy', 'type']]\ndf_energy_popular = df_spotify_history[['energy']].copy()\ndf_energy_popular['energy'] = df_energy_popular['energy'] / 100\ndf_energy_popular['type'] = 'Popular'\n\n\ndf_compare_energy = pd.concat([df_energy, df_energy_popular], axis=0)\n\n\nsns.boxplot(x=\"type\", y='energy', data=df_compare_energy)\nplt.xlabel('Popularity')\nplt.show()\n\n\n\n\nThis plot contrasts the energy distribution of top songs with that of all songs since 2010. Notably, the median energy of top songs is consistently higher than that of all songs. Furthermore, the energy of top songs tends to cluster within the 0.6-0.8 range, whereas the energy distribution for all songs is more dispersed.\n\ndf_tracks['year'] = df_tracks['release_date'].dt.year\ndf_spotify_history['energy'] = df_spotify_history['energy'] / 100 \n\n\nlen(df_tracks[df_tracks['year'] == 2010])\n\n8761\n\n\n\nlen(df_spotify_history[df_spotify_history['year'] == 2010])\n\n51\n\n\n\n# Bootstrap sampling the enegry column for each year beacuse lack of data\n\nbootstrap_samples = {'energy': [], 'year': []}\n\nfor year, group in df_spotify_history.groupby('year'):\n    for _ in range(100):  # bootstrap 100 times for each year\n        sample = group['energy'].sample(len(group), replace=True).tolist()\n        bootstrap_samples['year'] = bootstrap_samples['year'] + [year] * len(sample)\n        bootstrap_samples['energy'] = bootstrap_samples['energy'] + sample\n\n\ndf_bootstrap = pd.DataFrame(bootstrap_samples)\n\n\nsns.set_theme()\n\n# Create line plots for each genre or attribute\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=df_tracks[df_tracks['year'] &gt; 2010], x='year', y='energy', label='Energy_All', estimator='median')\nsns.lineplot(data=df_bootstrap, x='year', y='energy', label='Energy_Popular', estimator='median')\n\n# sns.lineplot(data=df_spotify_history, x='year', y='loudness', label='Loudness of Popular Song', estimator='median')\n# sns.lineplot(data=df_tracks[df_tracks['year'] &gt; 2010], x='year', y='loudness', label='Loudness of All Song', estimator='median')\n\n# ... Add other attributes as needed\n\n\n# Add title, labels, and legend\nplt.title('Distribution of Musical Attributes Over the Years')\nplt.xlabel('Year')\nplt.ylabel('Energy')\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\nThe plot illustrating the change in energy distribution of both top songs and other songs over the past decade offers intriguing insights. Notably, while the median energy value of top songs consistently surpasses that of all songs, this gap has been narrowing over time. Furthermore, there’s a discernible downward trend in song energy over the years. This could reflect a shift in musical preferences or a change in the characteristics of listeners.\n\n\n\n\n# load the data\ndf_listener = pd.read_csv('../data/01-modified-data/last.fm.data/users_cleaned.csv')\n\n\ndf_listener.head()\n\n\n\n\n\n\n\n\nuser_id\ncountry\nage\ngender\ncreation_time\n\n\n\n\n0\n2\nUK\n30-35\nm\n2002-10-29 01:00:00\n\n\n1\n6\nAT\n25-30\nn\n2003-07-23 02:00:00\n\n\n2\n14\nUK\n45-50\nm\n2003-02-18 21:44:13\n\n\n3\n15\nUS\n25-30\nm\n2003-02-24 03:30:33\n\n\n4\n20\nunknown\nNaN\nn\n2003-03-19 13:18:50\n\n\n\n\n\n\n\nThe dataset is mostly categroical data so there is no statistical summary for this dataset.\n\norder = df_listener['age'].value_counts().index\nsns.countplot(x='age', data=df_listener, order=order)\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\nWe are particularly interested in the age demographics. This plot reveals that the 20-25 year age group boasts the highest number of streaming users. In fact, the majority of users fall between the ages of 15 and 35. This underscores the significant influence of teenagers and Gen Z on the current streaming market.\n\n# load the data\ndf_listen_event = pd.read_csv('../data/01-modified-data/last.fm.data/listening_events_sample.csv')\n\n\n# construc the dataset\n\ndf_listen_info = df_listen_event.merge(df_listener, how='left', left_on='user_id', right_on='user_id')\n\n\n# drop the missing value\ndf_listen_info.dropna(inplace=True)\n\n\ndf_listen_info.head()\n\n\n\n\n\n\n\n\nuser_id\ntrack_id\nalbum_id\ntimestamp\ncountry\nage\ngender\ncreation_time\n\n\n\n\n0\n87459\n21395420\n17213208\n2020-02-07 12:59:11\nES\n15-20\nf\n2012-02-01 18:36:00\n\n\n1\n40265\n27546101\n20396068\n2020-01-21 16:34:24\nUS\n20-25\nf\n2009-11-15 12:17:35\n\n\n2\n29530\n32308561\n16367793\n2020-02-07 12:37:20\nBR\n20-25\nm\n2009-01-24 01:42:53\n\n\n3\n111719\n14820954\n10307272\n2020-03-07 05:32:22\nSE\n15-20\nm\n2012-05-02 23:13:26\n\n\n4\n33564\n9337136\n6639849\n2020-01-08 10:11:02\nHR\n20-25\nm\n2009-05-03 12:32:00\n\n\n\n\n\n\n\n\norder = df_listen_info['age'].value_counts().index\nsns.countplot(x='age', data=df_listen_info, order=order)\nplt.xticks(rotation=90)\nplt.ylabel('Listen Count')\nplt.show()\n\n\n\n\nFrom this graph, we can determine which group of listeners contributes to the most listening events. Interestingly, the order aligns closely with the number of consumers in each group.\n\nsns.countplot(x='gender', data=df_listener)\n\n&lt;Axes: xlabel='gender', ylabel='count'&gt;\n\n\n\n\n\nFrom this plot, it appears that males are the predominant users of the streaming service. This is somewhat surprising, as one might expect a more balanced number of male and female listeners. While there is a segment of users who do not disclose their gender, it doesn’t offset the noticeable gender imbalance.\n\nsns.countplot(x='gender', data=df_listen_info)\n\n&lt;Axes: xlabel='gender', ylabel='count'&gt;\n\n\n\n\n\nFrom the plot, it’s evident that males account for the majority of the listening events. This suggests that males are more inclined to listen to music frequently, as their representation in listening events is disproportionately higher than their actual numbers compared to females. Interestingly, individuals who choose not to disclose their gender appear to listen less frequently. Despite having a larger listener count, the number of listening events associated with this group is lower than that of females.\n\nsns.countplot(x='age', data=df_listener, hue='gender')\nplt.xticks(rotation=90)\n\n(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n [Text(0, 0, '30-35'),\n  Text(1, 0, '25-30'),\n  Text(2, 0, '45-50'),\n  Text(3, 0, '35-40'),\n  Text(4, 0, '20-25'),\n  Text(5, 0, '50-55'),\n  Text(6, 0, '40-45'),\n  Text(7, 0, '5-10'),\n  Text(8, 0, '15-20'),\n  Text(9, 0, '55-60'),\n  Text(10, 0, '60-65'),\n  Text(11, 0, '0-5'),\n  Text(12, 0, '70-75'),\n  Text(13, 0, '65-70'),\n  Text(14, 0, '10-15')])\n\n\n\n\n\nFrom the plot, it’s evident that males dominate the listener count across all age groups. Notably, the number of listeners who choose not to disclose their gender is increasingly prevalent among the 15-25 age group.\n\n# read the data\ndf_music = pd.read_csv('../data/01-modified-data/last.fm.data/tracks_cleaned.csv')\n\n\n# merge the data\ndf_listen_info_music = df_listen_info.merge(df_music, how='left', left_on='track_id', right_on='track_id')\n\n\ndf_listen_info_music.head()\n\n\n\n\n\n\n\n\nuser_id\ntrack_id\nalbum_id\ntimestamp\ncountry\nage\ngender\ncreation_time\nartist\ntrack\n\n\n\n\n0\n52710\n9299950\n16682758\n2020-02-14 09:52:25\nLT\nNaN\nf\n2010-08-18 19:52:37\nEERA\nChristine\n\n\n1\n4462\n41878638\n10034639\n2020-01-10 04:40:28\nCL\n25-30\nm\n2006-03-21 17:47:37\nLittle Mix\nTouch\n\n\n2\n43790\n38231384\n9469808\n2020-02-26 22:33:45\nIT\n15-20\nm\n2010-01-07 21:10:59\nBroken Social Scene\nSweetest Kill\n\n\n3\n62831\n18075725\n20196159\n2020-03-04 09:57:50\nID\n15-20\nm\n2011-03-20 05:19:10\nNeck Deep\nHeavy Lies\n\n\n4\n22870\n6576931\n8946033\n2020-03-10 23:50:23\nunknown\nNaN\nn\n2008-07-18 20:41:36\nImagine Dragons\nBeliever\n\n\n\n\n\n\n\n\ndf_listen_info_music.groupby('age')['artist'].apply(lambda x: x.value_counts().idxmax())\n\nage\n0-5                               米津玄師\n10-15                   Porcupine Tree\n15-20           White Noise Baby Sleep\n20-25                      Tame Impala\n25-30                          Beyoncé\n30-35                   Chris Kläfford\n35-40    Pen of Chaos et le Naheulband\n40-45                       • ukeboy •\n45-50                   The Beach Boys\n5-10                Alessandro Cortini\n50-55                       Pink Floyd\n55-60                      Count Basie\n60-65                              ДДТ\n65-70                      Ocean Grove\n70-75                          Sirenia\nName: artist, dtype: object\n\n\n\ndf_listen_info_music.groupby('age')['artist'].value_counts()['15-20']\n\nartist\nWhite Noise Baby Sleep         206\nBillie Eilish                   68\nBTS                             59\nTame Impala                     55\nGrimes                          49\n                              ... \nGodspeed You! Black Emperor      1\nGodfather of Harlem              1\nGod Help the Girl                1\nGod Dethroned                    1\nGo Radio                         1\nName: count, Length: 8445, dtype: int64\n\n\n\ndf_listen_info_music.groupby('age')['artist'].value_counts()['20-25']\n\nartist\nTame Impala      85\nRammstein        72\nDie drei ???     60\nSlipknot         60\nEminem           57\n                 ..\nKumbia Queers     1\nKumbhaka          1\nKula Shaker       1\nKuedo             1\nKubichek!         1\nName: count, Length: 13149, dtype: int64\n\n\n\ndf_listen_info_music.groupby('age')['artist'].value_counts()['25-30']\n\nartist\nBeyoncé          71\nHelene Bøksle    56\nLittle Mix       50\nAdam Sapphire    46\nDie drei ???     41\n                 ..\nAge Of Echoes     1\nAgaton Simon      1\nAgathocles        1\nAgar Agar         1\nAfterbirth        1\nName: count, Length: 7922, dtype: int64\n\n\nWhat these previous statistic is about each year group’s most played artist and their playcount. Becasue other age group has too small sample, here we only look at the three most major age group. It is interesting to see that some of the top ranked artist were not relevant to music, instead there are white noise and other sound product. It is clear that each group favors different type of aritist, though there are some overlapping. In the age group 25-30 Beyonce was the most played artist, while in the age group 20-25, it is Tame Impala. In the age group 15-20, it is Billie Eillish. This also shows how the age affect the music taste, where Beyonce was famous from the mid 1990s, and Tame Impala made their fame on 2010s while Billie Eillish was becoming golbal famous in about 2019. Apart from the affect of when the artist made their fame, it can also be observed that foreign languaage artist like BTS has ranked third in the group of age 15-20. This could be a sign of the globalization of music. It’s also interesting to see that the high ranked artist in the age group of 15-20 played most are generally cross genres, while in the age group of 25-30, the high ranked artist are more focus on a single genre.\n\n# Read the data\ndf_sound_info = pd.read_csv('../data/01-modified-data/last.fm.data/last_fm_track_info.csv')\n\n\n# merge the dataset\n\ndf_listen_sound = df_listen_info.merge(df_sound_info, how='left', left_on='track_id', right_on='track_id')\n\n\n# Drop the missing value\ndf_listen_sound.dropna(inplace=True)\n\n\ndf_listen_sound.head()\n\n\n\n\n\n\n\n\nuser_id\ntrack_id\nalbum_id\ntimestamp\ncountry\nage\ngender\ncreation_time\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nid\n\n\n\n\n1\n4462\n41878638\n10034639\n2020-01-10 04:40:28\nCL\n25-30\nm\n2006-03-21 17:47:37\n0.415\n0.104\n0.0\n-11.830\n1.0\n0.0426\n0.955000\n0.000004\n0.1040\n0.180\n82.006\n4g2WiijzSKzH8PApKDbadN\n\n\n2\n43790\n38231384\n9469808\n2020-02-26 22:33:45\nIT\n15-20\nm\n2010-01-07 21:10:59\n0.708\n0.384\n5.0\n-12.427\n1.0\n0.0302\n0.089300\n0.841000\n0.0961\n0.691\n87.961\n488u1IbVEsaC7fxjABWjHx\n\n\n5\n51493\n43590126\n444628\n2020-01-23 10:15:28\nUK\n20-25\nm\n2010-07-21 11:40:40\n0.486\n0.617\n5.0\n-7.115\n0.0\n0.0287\n0.095400\n0.000003\n0.1090\n0.417\n138.015\n1mea3bSkSGXuIRvnydlB5b\n\n\n13\n2194\n41200695\n19859102\n2020-03-09 19:06:27\nUK\n20-25\nm\n2005-08-22 06:45:25\n0.566\n0.869\n2.0\n-6.084\n1.0\n0.1000\n0.000916\n0.003460\n0.1140\n0.647\n178.113\n3C84jaEdYxiq8LC4jwYqj6\n\n\n21\n659\n6989555\n5566359\n2020-01-20 20:07:24\nDE\n20-25\nm\n2004-12-27 21:33:40\n0.376\n0.982\n5.0\n-3.779\n0.0\n0.0625\n0.000057\n0.000005\n0.1600\n0.150\n148.926\n6REc2Tq4G2RW5zKXtusTLF\n\n\n\n\n\n\n\n\ndf_listen_sound = df_listen_sound[(df_listen_sound['age'] == '15-20') | (df_listen_sound['age'] == '20-25') | (df_listen_sound['age'] == '25-30')]\n\n\nsns.violinplot(data=df_listen_sound, x=\"age\", y=\"tempo\")\n\n&lt;Axes: xlabel='age', ylabel='tempo'&gt;\n\n\n\n\n\n\nsns.violinplot(data=df_listen_sound, x=\"age\", y=\"danceability\")\n\n&lt;Axes: xlabel='age', ylabel='danceability'&gt;\n\n\n\n\n\nAn analysis of the previous two plots reveals insights into the preferences for danceability and energy across different age groups. For both features, the distributions for the age groups 25-30 and 20-25 exhibit similar shapes, suggesting comparable musical tastes within these age brackets. Conversely, the 15-20 age group displays a distinct pattern, characterized by a bimodal distribution with two clear peaks. This bimodality indicates diverse listening habits within this younger age bracket. Additionally, the median for the 15-20 age group is noticeably lower. A possible explanation for this deviation could be a subset of listeners in this age group who have a preference for tracks with characteristics akin to white noise which has 0 tempo and 0 danceability.\n\n\n\nIn conclusion, in this EDA process, we can gain following insight:\n\nPop music is still the king in the music, but it is losing its popularity. Hip hop is becoming more and more popular and also new genres are raising fast in the recent years.\nInstead of going more homogeneous, the popular songs are becoming more diverse in terms of some musical features include tempo and energy.\nThe tempo and energy in the popular songs are decreasing over time, while danceability is more and more popular.\nCompare with the popular songs in the streaming service, platform like tik tok prefers more energetic and danceable songs.\nThe features most positively realted to popularity is loudness, energy and explicit. The features most negatively related to popularity is acousticness and instrumentalness.\nThough energy is decreasing over time in popular songs, it is still higher than the median of overall songs. However, the gap is narrowing over time quickly.\n20-25 years old people is the age group has greatest number in streaming services, following by 15-20 years old and 25-30 years old.\nThere exist a serious unbanalce in gender of the users in streaming music services.\nPeople did not discolse their gender tend to have less listening events.\nThe music taste in different year group is noticable, even 15-20 and 20-25 has different top artist. Also, the music feature they prefer is slightly different too.\n\n\n\nBase on these conclusion from priliminary EDA, we can now refiend our hypothesis and research questions in the following way:\n\nAs the generation Z is becoming the main force of the music consumer, the music is becoming more diverse and less energetic while more danceable.\nThis could also lead to the decline of the popularity of pop music or other classic genres, instead, new genres which fuse those genres before are emerging and taking the market fast.\nGen Z will be more preferring to newer famous artists, and they are more open to music globalizaiton.\nPlatform like tik tok will be promoting more energetic and danceable songs in the future.\nIn the new generation, listeners could be clustered into more groups than before for haveing differnet music taste.\n\nIt is sure that more analysis need to be conduct after this EDA process."
  },
  {
    "objectID": "data_exploration.html#tools-used-in-data-exploration",
    "href": "data_exploration.html#tools-used-in-data-exploration",
    "title": "Data Exploration",
    "section": "",
    "text": "In this section, we will focus on exploring the data and understanding the data. For each dataset, we will try to find interesting trait or correlation between them or across dataset in order to answer the questions we proposed and refine our hypothesis. We will use the following tools to help us in this process: Pandas and Seaborn.\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# load the data\ndf_spotify_track = pd.read_csv('../data/01-modified-data/spotify_current_all.csv')\n\n\nprint(df_spotify_track.head())\n\n   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n0         0.864   0.556    2    -7.683     0       0.1940         0.255   \n1         0.802   0.832   11    -4.107     1       0.0434         0.311   \n2         0.552   0.702    9    -5.707     1       0.1570         0.117   \n3         0.841   0.738    7    -7.455     0       0.3070         0.520   \n4         0.628   0.523   11    -8.307     0       0.0946         0.701   \n\n   instrumentalness  liveness  valence    tempo                track_id  \\\n0          0.000004    0.1120    0.726   99.974  56y1jOTK0XSvJzVv9vHQBK   \n1          0.000000    0.0815    0.890  124.997  7x9aauaA9cu6tyfpHnqDLo   \n2          0.000021    0.1050    0.564  169.994  1BxfuPKGuaTgP7aM0Bbdwr   \n3          0.000000    0.0892    0.484  169.918  5RqSsdzTNPX1uzkmlHCFvK   \n4          0.002740    0.2190    0.416  169.982  5mjYQaktjmjcMKcUIcqz4s   \n\n               artist_ids                           track_name          name  \\\n0  5cj0lLjcoR7YOSnhnX0Po5                   Paint The Town Red      Doja Cat   \n1  6HaGTQPmzraVmaVxvz6EUc  Seven (feat. Latto) (Explicit Ver.)     Jung Kook   \n2  06HL4z0CvFAxyc27GXpf02                         Cruel Summer  Taylor Swift   \n3  790FomKkXshlbRYZFtlgla                                QLONA       KAROL G   \n4  7uMDnSZyUYNBPLhPMNuaM2                            Strangers   Kenya Grace   \n\n  broad_genre  \n0         pop  \n1         pop  \n2         pop  \n3       latin  \n4      others  \n\n\nHere is a statisiical summary of the dataset for popular songs.\n\nprint(df_spotify_track.describe())\n\n       danceability      energy         key    loudness        mode  \\\ncount    187.000000  187.000000  187.000000  187.000000  187.000000   \nmean       0.652583    0.640706    5.085561   -6.201134    0.647059   \nstd        0.145082    0.164622    3.582087    2.139002    0.479168   \nmin        0.351000    0.091100    0.000000  -17.665000    0.000000   \n25%        0.541000    0.535500    1.000000   -7.515000    0.000000   \n50%        0.647000    0.669000    5.000000   -5.745000    1.000000   \n75%        0.767000    0.762500    8.000000   -4.635500    1.000000   \nmax        0.971000    0.989000   11.000000   -2.278000    1.000000   \n\n       speechiness  acousticness  instrumentalness    liveness     valence  \\\ncount   187.000000    187.000000        187.000000  187.000000  187.000000   \nmean      0.085687      0.242592          0.014381    0.168414    0.507722   \nstd       0.083841      0.247013          0.076317    0.122816    0.244540   \nmin       0.025400      0.000390          0.000000    0.029700    0.034800   \n25%       0.036750      0.048200          0.000000    0.097050    0.318500   \n50%       0.053800      0.145000          0.000000    0.120000    0.488000   \n75%       0.088300      0.405000          0.000027    0.186000    0.696500   \nmax       0.397000      0.959000          0.629000    0.923000    0.964000   \n\n            tempo  \ncount  187.000000  \nmean   122.745358  \nstd     29.703599  \nmin     66.041000  \n25%     98.516500  \n50%    122.811000  \n75%    143.019500  \nmax    203.759000  \n\n\n\n\n\nsns.countplot(x='broad_genre', data=df_spotify_track)\nplt.xticks(rotation=90)\n\nplt.show()\n\n\n\n\nFrom this plot, it’s evident that pop music dominates the current top songs, followed by hip hop as the second most popular genre. Notably, the third most prevalent category is ‘others,’ which comprises songs that don’t fit neatly into established genres. This might hint at the growing trend of genreless music. However, a comparison with older songs is necessary to determine if this is a recent phenomenon.\n\n\n\n\n\n# load the data\ndf_spotify_history = pd.read_csv('../data/01-modified-data/top50MusicFrom2010-2019_cleaned.csv')\n\n\ndf_spotify_history.head()\n\n\n\n\n\n\n\n\ntitle\nartist\ngenre\nyear\nbeat\nenergy\ndanceability\nloudness\nvalence\nduration\nacousticness\nspeechiness\npopularity\nyear_group\n\n\n\n\n0\nHey, Soul Sister\nTrain\npop\n2010\n97\n89\n67\n-4\n80\n217\n19\n4\n83\n2010-2014\n\n\n1\nLove The Way You Lie\nEminem\nhip_hop_rap\n2010\n87\n93\n75\n-5\n64\n263\n24\n23\n82\n2010-2014\n\n\n2\nTiK ToK\nKesha\npop\n2010\n120\n84\n76\n-3\n71\n200\n10\n14\n80\n2010-2014\n\n\n3\nBad Romance\nLady Gaga\npop\n2010\n119\n92\n70\n-4\n71\n295\n0\n4\n79\n2010-2014\n\n\n4\nJust the Way You Are\nBruno Mars\npop\n2010\n109\n84\n64\n-5\n43\n221\n2\n4\n78\n2010-2014\n\n\n\n\n\n\n\nStatistics summary\n\ndf_spotify_history.describe()\n\n\n\n\n\n\n\n\nyear\nbeat\nenergy\ndanceability\nloudness\nvalence\nduration\nacousticness\nspeechiness\npopularity\n\n\n\n\ncount\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n603.000000\n\n\nmean\n2014.592040\n118.545605\n70.504146\n64.379768\n-5.578773\n52.225539\n224.674959\n14.326700\n8.358209\n66.520730\n\n\nstd\n2.607057\n24.795358\n16.310664\n13.378718\n2.798020\n22.513020\n34.130059\n20.766165\n7.483162\n14.517746\n\n\nmin\n2010.000000\n0.000000\n0.000000\n0.000000\n-60.000000\n0.000000\n134.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n2013.000000\n100.000000\n61.000000\n57.000000\n-6.000000\n35.000000\n202.000000\n2.000000\n4.000000\n60.000000\n\n\n50%\n2015.000000\n120.000000\n74.000000\n66.000000\n-5.000000\n52.000000\n221.000000\n6.000000\n5.000000\n69.000000\n\n\n75%\n2017.000000\n129.000000\n82.000000\n73.000000\n-4.000000\n69.000000\n239.500000\n17.000000\n9.000000\n76.000000\n\n\nmax\n2019.000000\n206.000000\n98.000000\n97.000000\n-2.000000\n98.000000\n424.000000\n99.000000\n48.000000\n99.000000\n\n\n\n\n\n\n\nWe can observe from the basic statistic that most of the statistic are quite similar to the ones in the currently hot songs.\n\ndf_spotify_history.columns\n\nIndex(['title', 'artist', 'genre', 'year', 'beat', 'energy', 'danceability',\n       'loudness', 'valence', 'duration', 'acousticness', 'speechiness',\n       'popularity', 'year_group'],\n      dtype='object')\n\n\n\ndf_histroy_melted  = df_spotify_history.melt(id_vars=['title', 'artist', 'year', 'beat', 'energy', 'danceability',\n       'loudness', 'valence', 'duration', 'acousticness', 'speechiness',\n       'popularity', 'year_group'], var_name='feature', value_name='value')\n\n\nsns.countplot(x='value', data=df_histroy_melted, hue='value')\nplt.xticks(rotation=90)\n\nplt.show()\n\n\n\n\nFrom this plot, it’s evident that the past decade witnessed an overwhelming dominance of pop music, comprising nearly all of the top songs. Electronic dance music emerged as the second most successful genre, with hip hop not even ranking third in popularity. There appears to have been some significant shifts in musical trends over the past decade. To highlight these changes more clearly, we should create another graph comparing the genre proportions from the last decade with those in the current top songs.\n\n# construst the dataset\ndf1 = df_spotify_history['genre'].value_counts() / df_spotify_history['genre'].value_counts().sum()\n\ndf2 = df_spotify_track['broad_genre'].value_counts() / df_spotify_track['broad_genre'].value_counts().sum()\n\n\ndf1 = pd.DataFrame(df1)\n# df1.set_index('genre').T\n\n\ndf1\ndf1.reset_index(inplace=True)\ndf1['type'] = 'history'\n\n\ndf2 = pd.DataFrame(df2)\ndf2.reset_index(inplace=True)\ndf2['type'] = 'current'\ndf2.rename(columns={'broad_genre':'genre'}, inplace=True)\n\n\ndf_genre_compare = pd.concat([df1, df2], axis=0)\n\n\nsns.barplot(x='genre', y='count', data=df_genre_compare, hue='type')\nplt.xticks(rotation=90)\nplt.ylabel('Percentage')\nplt.show()\n\n\n\n\nThis plot contrasts the proportion of each genre in the current top songs with those from the past decade. Notably, pop music’s share has seen a substantial decline, while other genres, particularly hip hop (now the second most prevalent genre in current top songs), have witnessed a surge. A significant observation is the minimal percentage of the ‘others’ category in past top songs, which now claims a notable share. This might be attributed to the emergence of hybrid genres like ‘gauze pop’ and ‘escape room’ that don’t fit conventional classifications and have only gained recognition recently. Latin music has also experienced a resurgence. In summary, there’s been a shift in genre distribution over the years, but a more detailed time-based analysis is needed to capture the full evolution.\n\ndf_genre_change = df_spotify_history.copy()\ndf_genre_change['count'] = 0\nfor i in range(len(df_genre_change)):\n    df_genre_change['count'][i] = len(df_genre_change[df_genre_change['year'] == df_genre_change['year'][i]])\n\n\ndf_genre_change['propotion'] = 1 / df_genre_change['count']\n\n\nsns.set_theme()\n\n# Create line plots for each genre or attribute\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=df_genre_change[df_genre_change['genre'] == 'pop'], x='year', y='propotion', label='Pop Proportion', estimator='sum')\nsns.lineplot(data=df_genre_change[df_genre_change['genre'] == 'hip_hop_rap'], x='year', y='propotion', label='Hip Hop Proportion', estimator='sum')\nsns.lineplot(data=df_genre_change[df_genre_change['genre'] == 'others'], x='year', y='propotion', label='Others Proportion', estimator='sum')\n# ... Add other attributes as needed\n\n# sns.lineplot(data=df_spotify_history, x='year', y='energy', label='Energy')\n\n# Add title, labels, and legend\nplt.title('Distribution of Musical Attributes Over the Years')\nplt.xlabel('Year')\nplt.ylabel('Value')\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\nIn the presented plot, we observe that the proportion of pop music within top songs each year doesn’t show a consistent decline but fluctuates over time. Nonetheless, the overarching trend suggests a decrease in the popularity of pop music. Hip hop music experienced a marked drop starting in 2010, stabilizing for several years thereafter. From 2015 onwards, hip hop’s proportion began to rise again. Furthermore, the “others” category, representing emerging genres not easily classified into traditional categories, began to see an uptick starting in 2018.\n\n\n\nTo do so, we will need to see the distribution of multipy musical features in the top songs over time.\n\nsns.set_theme()\n\n# Create line plots for each genre or attribute\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=df_spotify_history, x='year', y='beat', label='Beat', estimator='median')\nsns.lineplot(data=df_spotify_history, x='year', y='energy', label='Energy')\nsns.lineplot(data=df_spotify_history, x='year', y='danceability', label='Danceability')\n# ... Add other attributes as needed\n\n# sns.lineplot(data=df_spotify_history, x='year', y='energy', label='Energy')\n\n# Add title, labels, and legend\nplt.title('Distribution of Musical Attributes Over the Years')\nplt.xlabel('Year')\nplt.ylabel('Value')\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\nThe preceding graph illustrates the evolution of beats, danceability, and energy distributions over a decade. The line represents the median, while the shaded area encompasses the 25th to 75th percentiles. Contrary to expectations, the distributions of these features have not become more homogeneous over time. In fact, their distributions have remained relatively consistent. Notably, the range of beats has widened compared to earlier years. Regarding median values, there is a noticeable decline in both beats and energy over the years, while danceability has seen an upward trend.\n\n\n\n\n# load the data\ndf_tik_tok = pd.read_csv('../data/01-modified-data/tik_tok_cleaned.csv')\n\n\ndf_tik_tok.head()\n\n\n\n\n\n\n\n\ntrack_name\nartist\ntrack_pop\ndanceability\nenergy\nloudness\nmode\nkey\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\ntime_signature\nduration_ms\n\n\n\n\n0\nRunning Up That Hill (A Deal With God)\nKate Bush\n95\n0.629\n0.547\n-13.123\n0\n10\n0.0550\n0.7200\n0.003140\n0.0604\n0.197\n108.375\n4\n298933\n\n\n1\nAs It Was\nHarry Styles\n96\n0.520\n0.731\n-5.338\n0\n6\n0.0557\n0.3420\n0.001010\n0.3110\n0.662\n173.930\n4\n167303\n\n\n2\nSunroof\nNicky Youre\n44\n0.768\n0.716\n-5.110\n1\n10\n0.0404\n0.3500\n0.000000\n0.1500\n0.841\n131.430\n4\n163026\n\n\n3\nHeat Waves\nGlass Animals\n89\n0.761\n0.525\n-6.900\n1\n11\n0.0944\n0.4400\n0.000007\n0.0921\n0.531\n80.870\n4\n238805\n\n\n4\nAbout Damn Time\nLizzo\n92\n0.836\n0.743\n-6.305\n0\n10\n0.0656\n0.0995\n0.000000\n0.3350\n0.722\n108.966\n4\n191822\n\n\n\n\n\n\n\nStatistics summary\n\ndf_tik_tok.describe\n\n&lt;bound method NDFrame.describe of                                  track_name           artist  track_pop  \\\n0    Running Up That Hill (A Deal With God)        Kate Bush         95   \n1                                 As It Was     Harry Styles         96   \n2                                   Sunroof      Nicky Youre         44   \n3                                Heat Waves    Glass Animals         89   \n4                           About Damn Time            Lizzo         92   \n..                                      ...              ...        ...   \n258              The Less I Know The Better      Tame Impala         84   \n259                              Dandelions          Ruth B.         90   \n260           Jimmy Cooks (feat. 21 Savage)            Drake         92   \n261                            Good Looking  Suki Waterhouse         80   \n262                                 INFERNO        Sub Urban         71   \n\n     danceability  energy  loudness  mode  key  speechiness  acousticness  \\\n0           0.629   0.547   -13.123     0   10       0.0550      0.720000   \n1           0.520   0.731    -5.338     0    6       0.0557      0.342000   \n2           0.768   0.716    -5.110     1   10       0.0404      0.350000   \n3           0.761   0.525    -6.900     1   11       0.0944      0.440000   \n4           0.836   0.743    -6.305     0   10       0.0656      0.099500   \n..            ...     ...       ...   ...  ...          ...           ...   \n258         0.640   0.740    -4.083     1    4       0.0284      0.011500   \n259         0.609   0.692    -2.958     1    1       0.0259      0.015700   \n260         0.529   0.673    -4.711     1    0       0.1750      0.000307   \n261         0.377   0.558    -9.076     1    4       0.0299      0.078900   \n262         0.820   0.611    -5.020     0    9       0.1220      0.076600   \n\n     instrumentalness  liveness  valence    tempo  time_signature  duration_ms  \n0            0.003140    0.0604    0.197  108.375               4       298933  \n1            0.001010    0.3110    0.662  173.930               4       167303  \n2            0.000000    0.1500    0.841  131.430               4       163026  \n3            0.000007    0.0921    0.531   80.870               4       238805  \n4            0.000000    0.3350    0.722  108.966               4       191822  \n..                ...       ...      ...      ...             ...          ...  \n258          0.006780    0.1670    0.785  116.879               4       216320  \n259          0.000000    0.0864    0.454  116.959               3       233720  \n260          0.000002    0.0930    0.366  165.921               4       218365  \n261          0.000342    0.1250    0.267  149.971               3       214800  \n262          0.000025    0.0684    0.637  127.883               4       133134  \n\n[263 rows x 16 columns]&gt;\n\n\n\ndf_spotify_history.columns\n\nIndex(['title', 'artist', 'genre', 'year', 'beat', 'energy', 'danceability',\n       'loudness', 'valence', 'duration', 'acousticness', 'speechiness',\n       'popularity', 'year_group'],\n      dtype='object')\n\n\n\n# construst the dataset\ndf1 = df_spotify_track.copy()\ndf1['type'] = 'spotify'\ndf1.rename(columns={'broad_genre':'genre'}, inplace=True)\ndf2 = df_tik_tok.copy()\ndf2['type'] = 'tik_tok'\n\ndf_tik_spotify = pd.concat([df1, df2], axis=0)\n\n\ndf_tik_spotify.head()\n\n\n\n\n\n\n\n\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\n...\ntrack_id\nartist_ids\ntrack_name\nname\ngenre\ntype\nartist\ntrack_pop\ntime_signature\nduration_ms\n\n\n\n\n0\n0.864\n0.556\n2\n-7.683\n0\n0.1940\n0.255\n0.000004\n0.1120\n0.726\n...\n56y1jOTK0XSvJzVv9vHQBK\n5cj0lLjcoR7YOSnhnX0Po5\nPaint The Town Red\nDoja Cat\npop\nspotify\nNaN\nNaN\nNaN\nNaN\n\n\n1\n0.802\n0.832\n11\n-4.107\n1\n0.0434\n0.311\n0.000000\n0.0815\n0.890\n...\n7x9aauaA9cu6tyfpHnqDLo\n6HaGTQPmzraVmaVxvz6EUc\nSeven (feat. Latto) (Explicit Ver.)\nJung Kook\npop\nspotify\nNaN\nNaN\nNaN\nNaN\n\n\n2\n0.552\n0.702\n9\n-5.707\n1\n0.1570\n0.117\n0.000021\n0.1050\n0.564\n...\n1BxfuPKGuaTgP7aM0Bbdwr\n06HL4z0CvFAxyc27GXpf02\nCruel Summer\nTaylor Swift\npop\nspotify\nNaN\nNaN\nNaN\nNaN\n\n\n3\n0.841\n0.738\n7\n-7.455\n0\n0.3070\n0.520\n0.000000\n0.0892\n0.484\n...\n5RqSsdzTNPX1uzkmlHCFvK\n790FomKkXshlbRYZFtlgla\nQLONA\nKAROL G\nlatin\nspotify\nNaN\nNaN\nNaN\nNaN\n\n\n4\n0.628\n0.523\n11\n-8.307\n0\n0.0946\n0.701\n0.002740\n0.2190\n0.416\n...\n5mjYQaktjmjcMKcUIcqz4s\n7uMDnSZyUYNBPLhPMNuaM2\nStrangers\nKenya Grace\nothers\nspotify\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 21 columns\n\n\n\n\nsns.scatterplot(data=df_tik_spotify, x='danceability', y='energy', hue='type')\n\n&lt;Axes: xlabel='danceability', ylabel='energy'&gt;\n\n\n\n\n\nThis scatter plot depicts the relationship between song popularity and danceability for tracks popular on TikTok versus those favored on streaming services. Songs trending on TikTok tend to cluster around higher energy and danceability levels. This may be attributed to TikTok being a short-video platform, where more energetic and danceable songs are likely to thrive. In contrast, songs popular on streaming services appear more dispersed without an evident pattern. Notably, there are outliers: highly popular songs that aren’t particularly danceable, meriting further analysis.\n\n\n\n\n# Load the data\ndf_tracks = pd.read_csv('../data/00-raw-data/tracks.csv')\n\n\ncorr_1 = df_tracks.corr(method='pearson',numeric_only=True)  \n\nmask = np.triu(np.ones_like(corr_1, dtype=bool)) \nf, ax = plt.subplots(figsize=(11, 9)) \ncmap = sns.diverging_palette(230, 20, as_cmap=True) \n\nsns.heatmap(corr_1, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\nplt.show()\n\n\n\n\nFrom the heatmap, we can discern that most variables do not exhibit strong correlations with each other. Importantly, none of the values showcase a high correlation with popularity. The features most positively correlated with popularity are loudness and energy, with both having nearly identical correlation values. On the other end, acousticness is the most negatively correlated feature, followed closely by instrumentalness. Among all the features, energy and loudness are the most closely related, which makes sense as loudness often corresponds with a song’s energy. Based on these observations, we can delve deeper into the relationship between popularity, energy, and loudness. Additionally, both danceability and loudness have a positive correlation with a song’s valence.\n\ndf_tracks['release_date'] = pd.to_datetime(df_tracks['release_date'], errors='coerce')\n\n\ndf_2010 = df_tracks[df_tracks['release_date'].dt.year &gt;= 2010]\n\n\ndf_2010['type'] = 'Others'\n\n/var/folders/s0/4jmlz4bn2jv0712y1pv_xnbh0000gn/T/ipykernel_2997/361325449.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_2010['type'] = 'Others'\n\n\n\ndf_energy = df_2010[['energy', 'type']]\ndf_energy_popular = df_spotify_history[['energy']].copy()\ndf_energy_popular['energy'] = df_energy_popular['energy'] / 100\ndf_energy_popular['type'] = 'Popular'\n\n\ndf_compare_energy = pd.concat([df_energy, df_energy_popular], axis=0)\n\n\nsns.boxplot(x=\"type\", y='energy', data=df_compare_energy)\nplt.xlabel('Popularity')\nplt.show()\n\n\n\n\nThis plot contrasts the energy distribution of top songs with that of all songs since 2010. Notably, the median energy of top songs is consistently higher than that of all songs. Furthermore, the energy of top songs tends to cluster within the 0.6-0.8 range, whereas the energy distribution for all songs is more dispersed.\n\ndf_tracks['year'] = df_tracks['release_date'].dt.year\ndf_spotify_history['energy'] = df_spotify_history['energy'] / 100 \n\n\nlen(df_tracks[df_tracks['year'] == 2010])\n\n8761\n\n\n\nlen(df_spotify_history[df_spotify_history['year'] == 2010])\n\n51\n\n\n\n# Bootstrap sampling the enegry column for each year beacuse lack of data\n\nbootstrap_samples = {'energy': [], 'year': []}\n\nfor year, group in df_spotify_history.groupby('year'):\n    for _ in range(100):  # bootstrap 100 times for each year\n        sample = group['energy'].sample(len(group), replace=True).tolist()\n        bootstrap_samples['year'] = bootstrap_samples['year'] + [year] * len(sample)\n        bootstrap_samples['energy'] = bootstrap_samples['energy'] + sample\n\n\ndf_bootstrap = pd.DataFrame(bootstrap_samples)\n\n\nsns.set_theme()\n\n# Create line plots for each genre or attribute\nplt.figure(figsize=(10, 6))\nsns.lineplot(data=df_tracks[df_tracks['year'] &gt; 2010], x='year', y='energy', label='Energy_All', estimator='median')\nsns.lineplot(data=df_bootstrap, x='year', y='energy', label='Energy_Popular', estimator='median')\n\n# sns.lineplot(data=df_spotify_history, x='year', y='loudness', label='Loudness of Popular Song', estimator='median')\n# sns.lineplot(data=df_tracks[df_tracks['year'] &gt; 2010], x='year', y='loudness', label='Loudness of All Song', estimator='median')\n\n# ... Add other attributes as needed\n\n\n# Add title, labels, and legend\nplt.title('Distribution of Musical Attributes Over the Years')\nplt.xlabel('Year')\nplt.ylabel('Energy')\nplt.legend()\n\n# Show the plot\nplt.show()\n\n\n\n\nThe plot illustrating the change in energy distribution of both top songs and other songs over the past decade offers intriguing insights. Notably, while the median energy value of top songs consistently surpasses that of all songs, this gap has been narrowing over time. Furthermore, there’s a discernible downward trend in song energy over the years. This could reflect a shift in musical preferences or a change in the characteristics of listeners.\n\n\n\n\n# load the data\ndf_listener = pd.read_csv('../data/01-modified-data/last.fm.data/users_cleaned.csv')\n\n\ndf_listener.head()\n\n\n\n\n\n\n\n\nuser_id\ncountry\nage\ngender\ncreation_time\n\n\n\n\n0\n2\nUK\n30-35\nm\n2002-10-29 01:00:00\n\n\n1\n6\nAT\n25-30\nn\n2003-07-23 02:00:00\n\n\n2\n14\nUK\n45-50\nm\n2003-02-18 21:44:13\n\n\n3\n15\nUS\n25-30\nm\n2003-02-24 03:30:33\n\n\n4\n20\nunknown\nNaN\nn\n2003-03-19 13:18:50\n\n\n\n\n\n\n\nThe dataset is mostly categroical data so there is no statistical summary for this dataset.\n\norder = df_listener['age'].value_counts().index\nsns.countplot(x='age', data=df_listener, order=order)\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\nWe are particularly interested in the age demographics. This plot reveals that the 20-25 year age group boasts the highest number of streaming users. In fact, the majority of users fall between the ages of 15 and 35. This underscores the significant influence of teenagers and Gen Z on the current streaming market.\n\n# load the data\ndf_listen_event = pd.read_csv('../data/01-modified-data/last.fm.data/listening_events_sample.csv')\n\n\n# construc the dataset\n\ndf_listen_info = df_listen_event.merge(df_listener, how='left', left_on='user_id', right_on='user_id')\n\n\n# drop the missing value\ndf_listen_info.dropna(inplace=True)\n\n\ndf_listen_info.head()\n\n\n\n\n\n\n\n\nuser_id\ntrack_id\nalbum_id\ntimestamp\ncountry\nage\ngender\ncreation_time\n\n\n\n\n0\n87459\n21395420\n17213208\n2020-02-07 12:59:11\nES\n15-20\nf\n2012-02-01 18:36:00\n\n\n1\n40265\n27546101\n20396068\n2020-01-21 16:34:24\nUS\n20-25\nf\n2009-11-15 12:17:35\n\n\n2\n29530\n32308561\n16367793\n2020-02-07 12:37:20\nBR\n20-25\nm\n2009-01-24 01:42:53\n\n\n3\n111719\n14820954\n10307272\n2020-03-07 05:32:22\nSE\n15-20\nm\n2012-05-02 23:13:26\n\n\n4\n33564\n9337136\n6639849\n2020-01-08 10:11:02\nHR\n20-25\nm\n2009-05-03 12:32:00\n\n\n\n\n\n\n\n\norder = df_listen_info['age'].value_counts().index\nsns.countplot(x='age', data=df_listen_info, order=order)\nplt.xticks(rotation=90)\nplt.ylabel('Listen Count')\nplt.show()\n\n\n\n\nFrom this graph, we can determine which group of listeners contributes to the most listening events. Interestingly, the order aligns closely with the number of consumers in each group.\n\nsns.countplot(x='gender', data=df_listener)\n\n&lt;Axes: xlabel='gender', ylabel='count'&gt;\n\n\n\n\n\nFrom this plot, it appears that males are the predominant users of the streaming service. This is somewhat surprising, as one might expect a more balanced number of male and female listeners. While there is a segment of users who do not disclose their gender, it doesn’t offset the noticeable gender imbalance.\n\nsns.countplot(x='gender', data=df_listen_info)\n\n&lt;Axes: xlabel='gender', ylabel='count'&gt;\n\n\n\n\n\nFrom the plot, it’s evident that males account for the majority of the listening events. This suggests that males are more inclined to listen to music frequently, as their representation in listening events is disproportionately higher than their actual numbers compared to females. Interestingly, individuals who choose not to disclose their gender appear to listen less frequently. Despite having a larger listener count, the number of listening events associated with this group is lower than that of females.\n\nsns.countplot(x='age', data=df_listener, hue='gender')\nplt.xticks(rotation=90)\n\n(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]),\n [Text(0, 0, '30-35'),\n  Text(1, 0, '25-30'),\n  Text(2, 0, '45-50'),\n  Text(3, 0, '35-40'),\n  Text(4, 0, '20-25'),\n  Text(5, 0, '50-55'),\n  Text(6, 0, '40-45'),\n  Text(7, 0, '5-10'),\n  Text(8, 0, '15-20'),\n  Text(9, 0, '55-60'),\n  Text(10, 0, '60-65'),\n  Text(11, 0, '0-5'),\n  Text(12, 0, '70-75'),\n  Text(13, 0, '65-70'),\n  Text(14, 0, '10-15')])\n\n\n\n\n\nFrom the plot, it’s evident that males dominate the listener count across all age groups. Notably, the number of listeners who choose not to disclose their gender is increasingly prevalent among the 15-25 age group.\n\n# read the data\ndf_music = pd.read_csv('../data/01-modified-data/last.fm.data/tracks_cleaned.csv')\n\n\n# merge the data\ndf_listen_info_music = df_listen_info.merge(df_music, how='left', left_on='track_id', right_on='track_id')\n\n\ndf_listen_info_music.head()\n\n\n\n\n\n\n\n\nuser_id\ntrack_id\nalbum_id\ntimestamp\ncountry\nage\ngender\ncreation_time\nartist\ntrack\n\n\n\n\n0\n52710\n9299950\n16682758\n2020-02-14 09:52:25\nLT\nNaN\nf\n2010-08-18 19:52:37\nEERA\nChristine\n\n\n1\n4462\n41878638\n10034639\n2020-01-10 04:40:28\nCL\n25-30\nm\n2006-03-21 17:47:37\nLittle Mix\nTouch\n\n\n2\n43790\n38231384\n9469808\n2020-02-26 22:33:45\nIT\n15-20\nm\n2010-01-07 21:10:59\nBroken Social Scene\nSweetest Kill\n\n\n3\n62831\n18075725\n20196159\n2020-03-04 09:57:50\nID\n15-20\nm\n2011-03-20 05:19:10\nNeck Deep\nHeavy Lies\n\n\n4\n22870\n6576931\n8946033\n2020-03-10 23:50:23\nunknown\nNaN\nn\n2008-07-18 20:41:36\nImagine Dragons\nBeliever\n\n\n\n\n\n\n\n\ndf_listen_info_music.groupby('age')['artist'].apply(lambda x: x.value_counts().idxmax())\n\nage\n0-5                               米津玄師\n10-15                   Porcupine Tree\n15-20           White Noise Baby Sleep\n20-25                      Tame Impala\n25-30                          Beyoncé\n30-35                   Chris Kläfford\n35-40    Pen of Chaos et le Naheulband\n40-45                       • ukeboy •\n45-50                   The Beach Boys\n5-10                Alessandro Cortini\n50-55                       Pink Floyd\n55-60                      Count Basie\n60-65                              ДДТ\n65-70                      Ocean Grove\n70-75                          Sirenia\nName: artist, dtype: object\n\n\n\ndf_listen_info_music.groupby('age')['artist'].value_counts()['15-20']\n\nartist\nWhite Noise Baby Sleep         206\nBillie Eilish                   68\nBTS                             59\nTame Impala                     55\nGrimes                          49\n                              ... \nGodspeed You! Black Emperor      1\nGodfather of Harlem              1\nGod Help the Girl                1\nGod Dethroned                    1\nGo Radio                         1\nName: count, Length: 8445, dtype: int64\n\n\n\ndf_listen_info_music.groupby('age')['artist'].value_counts()['20-25']\n\nartist\nTame Impala      85\nRammstein        72\nDie drei ???     60\nSlipknot         60\nEminem           57\n                 ..\nKumbia Queers     1\nKumbhaka          1\nKula Shaker       1\nKuedo             1\nKubichek!         1\nName: count, Length: 13149, dtype: int64\n\n\n\ndf_listen_info_music.groupby('age')['artist'].value_counts()['25-30']\n\nartist\nBeyoncé          71\nHelene Bøksle    56\nLittle Mix       50\nAdam Sapphire    46\nDie drei ???     41\n                 ..\nAge Of Echoes     1\nAgaton Simon      1\nAgathocles        1\nAgar Agar         1\nAfterbirth        1\nName: count, Length: 7922, dtype: int64\n\n\nWhat these previous statistic is about each year group’s most played artist and their playcount. Becasue other age group has too small sample, here we only look at the three most major age group. It is interesting to see that some of the top ranked artist were not relevant to music, instead there are white noise and other sound product. It is clear that each group favors different type of aritist, though there are some overlapping. In the age group 25-30 Beyonce was the most played artist, while in the age group 20-25, it is Tame Impala. In the age group 15-20, it is Billie Eillish. This also shows how the age affect the music taste, where Beyonce was famous from the mid 1990s, and Tame Impala made their fame on 2010s while Billie Eillish was becoming golbal famous in about 2019. Apart from the affect of when the artist made their fame, it can also be observed that foreign languaage artist like BTS has ranked third in the group of age 15-20. This could be a sign of the globalization of music. It’s also interesting to see that the high ranked artist in the age group of 15-20 played most are generally cross genres, while in the age group of 25-30, the high ranked artist are more focus on a single genre.\n\n# Read the data\ndf_sound_info = pd.read_csv('../data/01-modified-data/last.fm.data/last_fm_track_info.csv')\n\n\n# merge the dataset\n\ndf_listen_sound = df_listen_info.merge(df_sound_info, how='left', left_on='track_id', right_on='track_id')\n\n\n# Drop the missing value\ndf_listen_sound.dropna(inplace=True)\n\n\ndf_listen_sound.head()\n\n\n\n\n\n\n\n\nuser_id\ntrack_id\nalbum_id\ntimestamp\ncountry\nage\ngender\ncreation_time\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nid\n\n\n\n\n1\n4462\n41878638\n10034639\n2020-01-10 04:40:28\nCL\n25-30\nm\n2006-03-21 17:47:37\n0.415\n0.104\n0.0\n-11.830\n1.0\n0.0426\n0.955000\n0.000004\n0.1040\n0.180\n82.006\n4g2WiijzSKzH8PApKDbadN\n\n\n2\n43790\n38231384\n9469808\n2020-02-26 22:33:45\nIT\n15-20\nm\n2010-01-07 21:10:59\n0.708\n0.384\n5.0\n-12.427\n1.0\n0.0302\n0.089300\n0.841000\n0.0961\n0.691\n87.961\n488u1IbVEsaC7fxjABWjHx\n\n\n5\n51493\n43590126\n444628\n2020-01-23 10:15:28\nUK\n20-25\nm\n2010-07-21 11:40:40\n0.486\n0.617\n5.0\n-7.115\n0.0\n0.0287\n0.095400\n0.000003\n0.1090\n0.417\n138.015\n1mea3bSkSGXuIRvnydlB5b\n\n\n13\n2194\n41200695\n19859102\n2020-03-09 19:06:27\nUK\n20-25\nm\n2005-08-22 06:45:25\n0.566\n0.869\n2.0\n-6.084\n1.0\n0.1000\n0.000916\n0.003460\n0.1140\n0.647\n178.113\n3C84jaEdYxiq8LC4jwYqj6\n\n\n21\n659\n6989555\n5566359\n2020-01-20 20:07:24\nDE\n20-25\nm\n2004-12-27 21:33:40\n0.376\n0.982\n5.0\n-3.779\n0.0\n0.0625\n0.000057\n0.000005\n0.1600\n0.150\n148.926\n6REc2Tq4G2RW5zKXtusTLF\n\n\n\n\n\n\n\n\ndf_listen_sound = df_listen_sound[(df_listen_sound['age'] == '15-20') | (df_listen_sound['age'] == '20-25') | (df_listen_sound['age'] == '25-30')]\n\n\nsns.violinplot(data=df_listen_sound, x=\"age\", y=\"tempo\")\n\n&lt;Axes: xlabel='age', ylabel='tempo'&gt;\n\n\n\n\n\n\nsns.violinplot(data=df_listen_sound, x=\"age\", y=\"danceability\")\n\n&lt;Axes: xlabel='age', ylabel='danceability'&gt;\n\n\n\n\n\nAn analysis of the previous two plots reveals insights into the preferences for danceability and energy across different age groups. For both features, the distributions for the age groups 25-30 and 20-25 exhibit similar shapes, suggesting comparable musical tastes within these age brackets. Conversely, the 15-20 age group displays a distinct pattern, characterized by a bimodal distribution with two clear peaks. This bimodality indicates diverse listening habits within this younger age bracket. Additionally, the median for the 15-20 age group is noticeably lower. A possible explanation for this deviation could be a subset of listeners in this age group who have a preference for tracks with characteristics akin to white noise which has 0 tempo and 0 danceability.\n\n\n\nIn conclusion, in this EDA process, we can gain following insight:\n\nPop music is still the king in the music, but it is losing its popularity. Hip hop is becoming more and more popular and also new genres are raising fast in the recent years.\nInstead of going more homogeneous, the popular songs are becoming more diverse in terms of some musical features include tempo and energy.\nThe tempo and energy in the popular songs are decreasing over time, while danceability is more and more popular.\nCompare with the popular songs in the streaming service, platform like tik tok prefers more energetic and danceable songs.\nThe features most positively realted to popularity is loudness, energy and explicit. The features most negatively related to popularity is acousticness and instrumentalness.\nThough energy is decreasing over time in popular songs, it is still higher than the median of overall songs. However, the gap is narrowing over time quickly.\n20-25 years old people is the age group has greatest number in streaming services, following by 15-20 years old and 25-30 years old.\nThere exist a serious unbanalce in gender of the users in streaming music services.\nPeople did not discolse their gender tend to have less listening events.\nThe music taste in different year group is noticable, even 15-20 and 20-25 has different top artist. Also, the music feature they prefer is slightly different too.\n\n\n\nBase on these conclusion from priliminary EDA, we can now refiend our hypothesis and research questions in the following way:\n\nAs the generation Z is becoming the main force of the music consumer, the music is becoming more diverse and less energetic while more danceable.\nThis could also lead to the decline of the popularity of pop music or other classic genres, instead, new genres which fuse those genres before are emerging and taking the market fast.\nGen Z will be more preferring to newer famous artists, and they are more open to music globalizaiton.\nPlatform like tik tok will be promoting more energetic and danceable songs in the future.\nIn the new generation, listeners could be clustered into more groups than before for haveing differnet music taste.\n\nIt is sure that more analysis need to be conduct after this EDA process."
  },
  {
    "objectID": "data_garther.html",
    "href": "data_garther.html",
    "title": "Data Gathering",
    "section": "",
    "text": "In this project’s data collection phase, I will utilize an API to gather recently generated data, enhancing our ability to analyze current trends. For data that is older or unavailable through the API, I will search for already done datasets on the internet and incorporate them into this project by downloading them.\n\n\n\n\n\nTo gain insights into how the trend has evolved over the past decades, I require some historical data. Retrieving this data from the API is a bit challenging. I will need to structure the chart to encompass the changes over these decades. Fortunately, there are available datasets on the internet due to the historical nature of the data. I have downloaded information about trending music over the past decade from the following link Top 50 Spotify music dataset (from 2010 to 2019 ) The data it collected is base on Spotify audio feature API.\nDataset Name : top50MusicFrom2010-2019.csv\nBasic Information:\n\ntotal 14 columns, 603 rows\n\n\n\n\n\n\n\nColumn Name\nNon-Null Count\nDtype\n\n\n\n\ntitle\n603 non-null\nobject\n\n\nartist\n603 non-null\nobject\n\n\nthe genre of the track\n603 non-null\nobject\n\n\nyear\n603 non-null\nint64\n\n\nBeats.Per.Minute -The tempo of the song\n603 non-null\nint64\n\n\nEnergy- The energy of a song - the higher the value, the more energtic\n603 non-null\nint64\n\n\nDanceability - The higher the value, the easier it is to dance to this song\n603 non-null\nint64\n\n\nLoudness/dB - The higher the value, the louder the song\n603 non-null\nint64\n\n\nLiveness - The higher the value, the more likely the song is a live recording\n603 non-null\nint64\n\n\nValence - The higher the value, the more positive mood for the song\n603 non-null\nint64\n\n\nLength - The duration of the song\n603 non-null\nint64\n\n\nAcousticness - The higher the value the more acoustic the song is\n603 non-null\nint64\n\n\nSpeechiness - The higher the value the more spoken word the song contains\n603 non-null\nint64\n\n\nPopularity- The higher the value the more popular the song is\n603 non-null\nint64\n\n\n\nA sample of the dataset:\n\nLink to the dataset: top50MusicFrom2010-2019.csv\n\n\n\nThe TikTok Developer API has stringent application requirements that I couldn’t fulfill. Consequently, I opted to obtain data on trending music from TikTok through this link TikTok Trending Tracks This dataset comprises 263 trending songs on TikTok in the year 2022. Each song’s information was sourced from Spotify, enabling consistant analysis alongside the data collected using the Spotify API.\nDataset Name : TikTok_songs_2022.csv\nBasic Information:\n\ntotal 14 columns, 603 rows\n\n\nColumn\nNon-Null Count\nDtype\n\n\n\n\ntrack_name\n263 non-null\nobject\n\n\nartist_name\n263 non-null\nobject\n\n\nartist_pop\n263 non-null\nint64\n\n\nalbum\n263 non-null\nobject\n\n\ntrack_pop\n263 non-null\nint64\n\n\ndanceability\n263 non-null\nfloat64\n\n\nenergy\n263 non-null\nfloat64\n\n\nloudness\n263 non-null\nfloat64\n\n\nmode\n263 non-null\nint64\n\n\nkey\n263 non-null\nint64\n\n\nspeechiness\n263 non-null\nfloat64\n\n\nacousticness\n263 non-null\nfloat64\n\n\ninstrumentalness\n263 non-null\nfloat64\n\n\nliveness\n263 non-null\nfloat64\n\n\nvalence\n263 non-null\nfloat64\n\n\ntempo\n263 non-null\nfloat64\n\n\ntime_signature\n263 non-null\nint64\n\n\nduration_ms\n263 non-null\nint64\n\n\n\nA sample of the dataset:\n\nLink to the dataset: TikTok_songs_2022.csv\n\n\n\nI also required data to analyze user behaviors in order to gain insights how the musics are consumed. However, user data is often confidential and not disclosed to public. To address this, I discovered an open dataset about last.fm users listening events, which includes some anonymized user personal information. The link for this dataset is LFM-2b Dataset. This dataset comprises three key components:\nThe first component consists of three TSV files.\n\n\nOne file contains anonymized user personal information such as gender and age.\nThe second file stores track information, including track ID and artist names.\nThe third file contains details about listening events, listing each user event, including the timestamp, the user who listened to the music, and the specific track they played.\n\n\nSome of the dataset are quite messy and unable to be read directly using pandas with mix sepeartors. So I could only show the samples of the datastes here.\nDataset Name : users.tsv\nSample of the dataset: \nLink to the dataset: users.tsv\nDataset Name : tracks.tsv\nSample of the dataset:\n\nLink to the dataset: tracks.tsv\nDataset Name : listening_events.tsv\nSample of the dataset:\n\nLink to the dataset: listening_events.tsv\n\n\n\n\n\n\nTo gain a complete understanding of recent developments in the music industry and to address the more recently produced trending musics, I initiated the project by collecting data on today’s trending songs. For this purpose, I employed the Spotify API to gather music information from sources such as the ‘U.S. Top Fifty’ playlist, the Billboard Top 100, and the Billboard chart for the year 2022. The three dataset are in the same format, so I will only show the sample of one of them for the sake of brevity.\nAPI Endpoint for this sample: https://api.spotify.com/v1/playlists/6UeSakyzhiEt4NB3UAd6NQ/tracks\nDataset Name : billboard_features.csv\nBasic information:\n\ntotal 21 columns, 100 rows\n\n\nColumn\nNon-Null Count\nDtype\n\n\n\n\ndanceability\n100 non-null\nfloat64\n\n\nenergy\n100 non-null\nfloat64\n\n\nkey\n100 non-null\nint64\n\n\nloudness\n100 non-null\nfloat64\n\n\nmode\n100 non-null\nint64\n\n\nspeechiness\n100 non-null\nfloat64\n\n\nacousticness\n100 non-null\nfloat64\n\n\ninstrumentalness\n100 non-null\nfloat64\n\n\nliveness\n100 non-null\nfloat64\n\n\nvalence\n100 non-null\nfloat64\n\n\ntempo\n100 non-null\nfloat64\n\n\ntype\n100 non-null\nobject\n\n\nid\n100 non-null\nobject\n\n\nuri\n100 non-null\nobject\n\n\ntrack_href\n100 non-null\nobject\n\n\nanalysis_url\n100 non-null\nobject\n\n\nduration_ms\n100 non-null\nint64\n\n\ntime_signature\n100 non-null\nint64\n\n\nartist_ids\n100 non-null\nobject\n\n\ntrack_name\n100 non-null\nobject\n\n\n\nA sample of the dataset:\n\nLink to the dataset: billboard_features.csv\n\n\n\nAnother crucial aspect in this project is the lyric of the songs which Spotify does not provide. To address this, I used Genius API to obtain the lyrics data. Specifically, I utilized the ‘lyricsgenius’ package in Python to fetch the lyrics for each song by their names. I fetched the lyrcis for each songs in the dataset above, so there will be three lyrics dataset which follows the same format. I will only show the sample of one of them for the sake of brevity.\nDataset Name : genius_lyrics\nBasic information:\n\ntotal 3 columns, 100 rows\n\n\nColumn\nNon-Null Count\nDtype\n\n\n\n\ntrack_id\n100 non-null\nobject\n\n\nname\n100 non-null\nobject\n\n\nlyrics\n100 non-null\nobject\n\n\n\nA sample of the dataset:\n\nLink to the dataset: genius_lyrics.csv\n\n\n\nThe last.fm data, as mentioned in the previous section, provided valuable user information and activity data. However, the track data lacked audio analytical factors, consisting only of artist and track names, which is unable to conduct any meaningful analysis. To complement this, I utilized the Spotify API to search for and retrieve audio features for the music. The resulting dataset is stored in a JSON file. My plan was to collect the top 10,000 songs’ audio feature it is still under the process of collecting becasue Spotify API does have a rate limit, so it takes time to collect all the data. For now, the dataset only consist part of it, but the format is finalized.\nDataset Name : sample_track_info.json\nAPI Endpoint for searching track id: https://api.spotify.com/v1/search/track:{track_name}%20artist:{artist_name}\nAPI Endpoint for getting the audio feature: https://api.spotify.com/v1/audio-features/{track_id}\nBasic information:\n\ntotal 19 columns, 5700 rows\n\n\nColumn\nNon-Null Count\nDtype\n\n\n\n\ndanceability\n5700 non-null\nfloat64\n\n\nenergy\n5700 non-null\nfloat64\n\n\nkey\n5700 non-null\nint64\n\n\nloudness\n5700 non-null\nfloat64\n\n\nmode\n5700 non-null\nint64\n\n\nspeechiness\n5700 non-null\nfloat64\n\n\nacousticness\n5700 non-null\nfloat64\n\n\ninstrumentalness\n5700 non-null\nfloat64\n\n\nliveness\n5700 non-null\nfloat64\n\n\nvalence\n5700 non-null\nfloat64\n\n\ntempo\n5700 non-null\nfloat64\n\n\ntype\n5700 non-null\nobject\n\n\nid\n5700 non-null\nobject\n\n\nuri\n5700 non-null\nobject\n\n\ntrack_href\n5700 non-null\nobject\n\n\nanalysis_url\n5700 non-null\nobject\n\n\nduration_ms\n5700 non-null\nint64\n\n\ntime_signature\n5700 non-null\nint64\n\n\ntrack_id\n5700 non-null\nint64\n\n\n\nA sample of the dataset:\n\nLink to the dataset: sample_track_info.json"
  },
  {
    "objectID": "data_garther.html#api-data-gathering",
    "href": "data_garther.html#api-data-gathering",
    "title": "Data Gathering",
    "section": "",
    "text": "In this project’s data collection phase, I will utilize an API to gather recently generated data, enhancing our ability to analyze current trends. For data that is older or unavailable through the API, I will search for already done datasets on the internet and incorporate them into this project by downloading them.\n\n\n\n\n\nTo gain insights into how the trend has evolved over the past decades, I require some historical data. Retrieving this data from the API is a bit challenging. I will need to structure the chart to encompass the changes over these decades. Fortunately, there are available datasets on the internet due to the historical nature of the data. I have downloaded information about trending music over the past decade from the following link Top 50 Spotify music dataset (from 2010 to 2019 ) The data it collected is base on Spotify audio feature API.\nDataset Name : top50MusicFrom2010-2019.csv\nBasic Information:\n\ntotal 14 columns, 603 rows\n\n\n\n\n\n\n\nColumn Name\nNon-Null Count\nDtype\n\n\n\n\ntitle\n603 non-null\nobject\n\n\nartist\n603 non-null\nobject\n\n\nthe genre of the track\n603 non-null\nobject\n\n\nyear\n603 non-null\nint64\n\n\nBeats.Per.Minute -The tempo of the song\n603 non-null\nint64\n\n\nEnergy- The energy of a song - the higher the value, the more energtic\n603 non-null\nint64\n\n\nDanceability - The higher the value, the easier it is to dance to this song\n603 non-null\nint64\n\n\nLoudness/dB - The higher the value, the louder the song\n603 non-null\nint64\n\n\nLiveness - The higher the value, the more likely the song is a live recording\n603 non-null\nint64\n\n\nValence - The higher the value, the more positive mood for the song\n603 non-null\nint64\n\n\nLength - The duration of the song\n603 non-null\nint64\n\n\nAcousticness - The higher the value the more acoustic the song is\n603 non-null\nint64\n\n\nSpeechiness - The higher the value the more spoken word the song contains\n603 non-null\nint64\n\n\nPopularity- The higher the value the more popular the song is\n603 non-null\nint64\n\n\n\nA sample of the dataset:\n\nLink to the dataset: top50MusicFrom2010-2019.csv\n\n\n\nThe TikTok Developer API has stringent application requirements that I couldn’t fulfill. Consequently, I opted to obtain data on trending music from TikTok through this link TikTok Trending Tracks This dataset comprises 263 trending songs on TikTok in the year 2022. Each song’s information was sourced from Spotify, enabling consistant analysis alongside the data collected using the Spotify API.\nDataset Name : TikTok_songs_2022.csv\nBasic Information:\n\ntotal 14 columns, 603 rows\n\n\nColumn\nNon-Null Count\nDtype\n\n\n\n\ntrack_name\n263 non-null\nobject\n\n\nartist_name\n263 non-null\nobject\n\n\nartist_pop\n263 non-null\nint64\n\n\nalbum\n263 non-null\nobject\n\n\ntrack_pop\n263 non-null\nint64\n\n\ndanceability\n263 non-null\nfloat64\n\n\nenergy\n263 non-null\nfloat64\n\n\nloudness\n263 non-null\nfloat64\n\n\nmode\n263 non-null\nint64\n\n\nkey\n263 non-null\nint64\n\n\nspeechiness\n263 non-null\nfloat64\n\n\nacousticness\n263 non-null\nfloat64\n\n\ninstrumentalness\n263 non-null\nfloat64\n\n\nliveness\n263 non-null\nfloat64\n\n\nvalence\n263 non-null\nfloat64\n\n\ntempo\n263 non-null\nfloat64\n\n\ntime_signature\n263 non-null\nint64\n\n\nduration_ms\n263 non-null\nint64\n\n\n\nA sample of the dataset:\n\nLink to the dataset: TikTok_songs_2022.csv\n\n\n\nI also required data to analyze user behaviors in order to gain insights how the musics are consumed. However, user data is often confidential and not disclosed to public. To address this, I discovered an open dataset about last.fm users listening events, which includes some anonymized user personal information. The link for this dataset is LFM-2b Dataset. This dataset comprises three key components:\nThe first component consists of three TSV files.\n\n\nOne file contains anonymized user personal information such as gender and age.\nThe second file stores track information, including track ID and artist names.\nThe third file contains details about listening events, listing each user event, including the timestamp, the user who listened to the music, and the specific track they played.\n\n\nSome of the dataset are quite messy and unable to be read directly using pandas with mix sepeartors. So I could only show the samples of the datastes here.\nDataset Name : users.tsv\nSample of the dataset: \nLink to the dataset: users.tsv\nDataset Name : tracks.tsv\nSample of the dataset:\n\nLink to the dataset: tracks.tsv\nDataset Name : listening_events.tsv\nSample of the dataset:\n\nLink to the dataset: listening_events.tsv\n\n\n\n\n\n\nTo gain a complete understanding of recent developments in the music industry and to address the more recently produced trending musics, I initiated the project by collecting data on today’s trending songs. For this purpose, I employed the Spotify API to gather music information from sources such as the ‘U.S. Top Fifty’ playlist, the Billboard Top 100, and the Billboard chart for the year 2022. The three dataset are in the same format, so I will only show the sample of one of them for the sake of brevity.\nAPI Endpoint for this sample: https://api.spotify.com/v1/playlists/6UeSakyzhiEt4NB3UAd6NQ/tracks\nDataset Name : billboard_features.csv\nBasic information:\n\ntotal 21 columns, 100 rows\n\n\nColumn\nNon-Null Count\nDtype\n\n\n\n\ndanceability\n100 non-null\nfloat64\n\n\nenergy\n100 non-null\nfloat64\n\n\nkey\n100 non-null\nint64\n\n\nloudness\n100 non-null\nfloat64\n\n\nmode\n100 non-null\nint64\n\n\nspeechiness\n100 non-null\nfloat64\n\n\nacousticness\n100 non-null\nfloat64\n\n\ninstrumentalness\n100 non-null\nfloat64\n\n\nliveness\n100 non-null\nfloat64\n\n\nvalence\n100 non-null\nfloat64\n\n\ntempo\n100 non-null\nfloat64\n\n\ntype\n100 non-null\nobject\n\n\nid\n100 non-null\nobject\n\n\nuri\n100 non-null\nobject\n\n\ntrack_href\n100 non-null\nobject\n\n\nanalysis_url\n100 non-null\nobject\n\n\nduration_ms\n100 non-null\nint64\n\n\ntime_signature\n100 non-null\nint64\n\n\nartist_ids\n100 non-null\nobject\n\n\ntrack_name\n100 non-null\nobject\n\n\n\nA sample of the dataset:\n\nLink to the dataset: billboard_features.csv\n\n\n\nAnother crucial aspect in this project is the lyric of the songs which Spotify does not provide. To address this, I used Genius API to obtain the lyrics data. Specifically, I utilized the ‘lyricsgenius’ package in Python to fetch the lyrics for each song by their names. I fetched the lyrcis for each songs in the dataset above, so there will be three lyrics dataset which follows the same format. I will only show the sample of one of them for the sake of brevity.\nDataset Name : genius_lyrics\nBasic information:\n\ntotal 3 columns, 100 rows\n\n\nColumn\nNon-Null Count\nDtype\n\n\n\n\ntrack_id\n100 non-null\nobject\n\n\nname\n100 non-null\nobject\n\n\nlyrics\n100 non-null\nobject\n\n\n\nA sample of the dataset:\n\nLink to the dataset: genius_lyrics.csv\n\n\n\nThe last.fm data, as mentioned in the previous section, provided valuable user information and activity data. However, the track data lacked audio analytical factors, consisting only of artist and track names, which is unable to conduct any meaningful analysis. To complement this, I utilized the Spotify API to search for and retrieve audio features for the music. The resulting dataset is stored in a JSON file. My plan was to collect the top 10,000 songs’ audio feature it is still under the process of collecting becasue Spotify API does have a rate limit, so it takes time to collect all the data. For now, the dataset only consist part of it, but the format is finalized.\nDataset Name : sample_track_info.json\nAPI Endpoint for searching track id: https://api.spotify.com/v1/search/track:{track_name}%20artist:{artist_name}\nAPI Endpoint for getting the audio feature: https://api.spotify.com/v1/audio-features/{track_id}\nBasic information:\n\ntotal 19 columns, 5700 rows\n\n\nColumn\nNon-Null Count\nDtype\n\n\n\n\ndanceability\n5700 non-null\nfloat64\n\n\nenergy\n5700 non-null\nfloat64\n\n\nkey\n5700 non-null\nint64\n\n\nloudness\n5700 non-null\nfloat64\n\n\nmode\n5700 non-null\nint64\n\n\nspeechiness\n5700 non-null\nfloat64\n\n\nacousticness\n5700 non-null\nfloat64\n\n\ninstrumentalness\n5700 non-null\nfloat64\n\n\nliveness\n5700 non-null\nfloat64\n\n\nvalence\n5700 non-null\nfloat64\n\n\ntempo\n5700 non-null\nfloat64\n\n\ntype\n5700 non-null\nobject\n\n\nid\n5700 non-null\nobject\n\n\nuri\n5700 non-null\nobject\n\n\ntrack_href\n5700 non-null\nobject\n\n\nanalysis_url\n5700 non-null\nobject\n\n\nduration_ms\n5700 non-null\nint64\n\n\ntime_signature\n5700 non-null\nint64\n\n\ntrack_id\n5700 non-null\nint64\n\n\n\nA sample of the dataset:\n\nLink to the dataset: sample_track_info.json"
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Cleaning Spotify data of current trending songs:\n\n\nFirst I merge the data from three files (global_top_50.csv, billboard_features.csv, billboard_2022_features.csv) containing recent trending music in Spotify. Because they have the same format, I just need to combine them into a data frame.\nThere are a lot of not useful colums containing various urls as shown in the picture below. Drop the unnecessary columns like time_signature, analysis_url, track_href… to lower the data dimension. Before cleaning the columns looks like this:  After cleaning, noticed all the url columns are gone: \nCheck if there are any missing values, in this case, the dataset does not contain any, like the picture below: \nIn some case there are multipy artists for For the case where a track contain more than one artiest, just leave the prominent one to keep constant with other datasets.\nAn example of multiple artists (the red words are the artist ids): \nSince we are concern about the genre information for each song, add genre column which get data from the artist dataset, using the genre information from the artist dataset spotify_artist.csv.\nColumns after adding genre information: \n\nA sample of the cleaned dataset:\n\n\n\nsample\n\n\nLink to the cleaned dataset: spotify_current_all.csv\n\n\n\nClean historical trending musics:\n\n\nFirst, I noticed the column name is too redundancy, so I changed them so they are easier to retrieve and also consistent with the previous contemporary music data.\nColumn names Before cleaning:  After cleaning: \nCheck if there are any missing values, in this case, there are no missing values \nDrop the ‘liveness’ column because it is not relevant to the music itself.\nColumns after dropping the ‘liveness’ column: \nThere are too many genres in this dataset like shown in the picture below. To make it better for analysis, I map each specific genre using regular expression and hard code mapping.\nBefore mapping:  After mapping:\n\nCreate a year binning over the year column. Make the bin width of 5 years.\nBefore binning, the year column is like this, just separate years indicate the year of the song:\n$ year        : num [1:603] 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 ... After bining, the years is turned to two groups, like shown in the picture below:\n\n\nHere is a sample of the cleaned dataset: \nLink to the cleaned dataset: top50MusicFrom2010-2019_cleaned.csv\n\n\n\nClean the artist dataset:\n\n\nThere are too many not useful columns like various urls so drop them first. Also, each artist has 8 columns of genre, to keep it consistent with other datasets, drop the genre columns and only keep the priority genre. However, the priority genre is either stored in the genres1 column or the genres column, depending on how many genres the artist have, so here I only drop the genres2 to genres8 columns.\nColumn names before dropping:\n Columns after dropping:\n\nCombine the genre1 column and genre column to create the final genre column for each artist.\nBefore cleaning (the green and red colored words are the genres, this only shows a part of the genres since it got 8 columns of genre for each artist):\n After cleaning:\n\nIt can be observe from the previous picture a lot of artist has no genre information. Fill these missing values in the genre field using the word ‘undefined’.\nAfter filling the missing values:\n\nMap the specific genre to a broader genre to reduce the number of genres. This step was done similarly to the last dataset.\nBefore mapping (there are 75 genres in total, here I onnly shows a part of them):\n After mapping:\n\n\nHere is a sample of the cleaned dataset:\n\nLink to the cleaned dataset: spotify_artist.csv\n\n\n\nClean lyrics dataset:\n\n\nMerge all the lyrics dataset (genius_lyrics_2022.csv, genius_lyrics_global.csv, genius_lyrics.csv) into one data frame.\nBy checking, there are duplicates exists, dropping all the duplicates, since there are duplicates in different trending list. Print the duplicates rows before dropping:  After dropping duplicates, use df.duplicated().sum() to check if there are any duplicates, the result should be 0.\nDrop the rows with missing lyrics, since it will serve no use in analysis.\nBefore dropping: \nAfter dropping, use df.isnull().sum() to check if there are any missing values, the result should be 0.\nRemove the indication or introduction before each song’s lyric using regular expression.\nBefore removing:\n\nAfter removing (notice the [chours] and [verse] are gone, also the lyric’s information like contributors is removed as well):\n\nRemove all the unprintable and puncuation and non-English characters since it will not be used in the analysis.\nBefore removing:\n\nAfter removing(notice the lyric only contains non-English characters are now empty):\n\nIn this picture, we can see all the punctuations are gone: \nDrop all the new empty rows using df.dropna().\nCreate a bag of word from the lyrics column using CountVectorizer and use stop_words='english' parameter to remove the stop words and drop the original lyric column.\n\nHere is a sample of the cleaned dataset:\n\nLink to the cleaned dataset: genius_lyrics_cleaned\n\n\n\nCleaning TikTok dataset\n\n\nCheck for missing value using df.isnull().sum(), there are no missing values in this dataset as shown in the picture below:\n\nCheck for duplicate value using duplicates = df_[df_tik.duplicated()], it will return a empty dataframe, showing there are no duplicates in this dataset.\nDrop not useful columns, album name and artist popularity.\nBefore dropping:\n\nAfter dropping, notice the colums dropped are gone:\n Rename the columns for better data retrieval. After rename the colums, the columns names are like this:\n\n\nHere is a sample of the cleaned dataset:\n\nLink to the cleaned dataset: tiktok_cleaned.csv\n\n\n\nClean last.fm listening events dataset\n\n\nThe listening events in the dataset are too many, 30,357,786 columns in total, which could cause trouble in analysis. So here we only randomly sample 10000 listening events from the dataset (This is just an initial decision, it could chagne as the analysis went on)\nThe columns in the dataset are squashed into one column, so i need to split it by the separator ’.\nBefore splitting:\n\nAfter splitting, notice the columns are now separated into four columns:\n\nCheck if there are any missing values in the sample dataset using colSums(is.na(df)), in this case there are no missing values in the sample dataset.\n\nHere is a sample of the cleaned dataset:\n\nLink to the cleaned dataset: listening_events_sample.csv\n\n\n\nClean last.fm track datasets\n\n\nThe data itself is encode in tsv format, but the separator is not consistent where sometime it is tab and sometimes it is spaces. So it is impossible to read directly using pandas. First, I read the file as plain text and change all the separator to comma, then save the result as a .csv file.\nBefore changing the separator: \nAfter changing the separator and save as .csv file:\n\nThere are a lot of columns in the track name are clearly unreadable, consisting of ‘!’ Or simply dots and white spaces or something like Remove all the columns only consist with these with regular expression.\nBefore removing:  After removing (notice the meaningless columns are turned to NA): \nDrop all the rows with missing values and unprintable vlaues and write the cleaned dataset to a new csv file.\n\nA sample of the cleaned dataset:\n\nLink to the cleaned dataset: tracks_cleaned.csv\n\n\n\nClean last.fm user dataset\n\n\nPlenty of users’s country information is missing, I replace all the empty value using the word ‘unknown’\nData before cleaning: \nIt is worth noticing that there are a lot of -1 ages in this dataset. It consists of 0.3% of the data. With so much missing value, whether chaning to the mean or media will introduce huge change to the variance of the data (variance dropping 23 if using media, more if using mean). So I decided the replacing of the missing value need to be done after more analysis in the next stage.\nAlso from the distribution we can see that there are very few data has age over 80, so remove them as well. This is about 0.03% of the data.\nHere is the distribution of the age in the dataset: \nUsing bining to create a 5 years range bin in the age values for better analysis.\nBefore binning, the frequency of the age is like this: \nAfter binning: \n\nHere is a sample of the cleaned dataset:\n\nLink to the cleaned dataset: users_cleaned.csv"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "An Analysis of the Trend in Today’s Music Industry",
    "section": "",
    "text": "Music, arguably invented earlier than language, has been an important part of human civilization. During the years music has been constantly evolving and could be serve as a reflection on the society of its time. The pace of the evolve though, is noticeable accelerated in recent years. With the new technology and the rise of Gen Z as main consumer, nearly every aspect of the industry has changed significantly. New genres such as sped-ups are emerging in a speedy way and streaming service has completely changed the way consumer access to music and hence their consuming behaviors. More powerful AI tools help independent artist to produce quality works, while crossover between music and video and games become a new force due to platform like TikTok. To understand the trend in contemporary music offers not only a great commercial value but also provide us an insight into modern society and this new generation. In this project, we will employ a data driven method to depict the revolution in music industry and try to predict its future."
  },
  {
    "objectID": "introduction.html#project-introduction",
    "href": "introduction.html#project-introduction",
    "title": "An Analysis of the Trend in Today’s Music Industry",
    "section": "",
    "text": "Music, arguably invented earlier than language, has been an important part of human civilization. During the years music has been constantly evolving and could be serve as a reflection on the society of its time. The pace of the evolve though, is noticeable accelerated in recent years. With the new technology and the rise of Gen Z as main consumer, nearly every aspect of the industry has changed significantly. New genres such as sped-ups are emerging in a speedy way and streaming service has completely changed the way consumer access to music and hence their consuming behaviors. More powerful AI tools help independent artist to produce quality works, while crossover between music and video and games become a new force due to platform like TikTok. To understand the trend in contemporary music offers not only a great commercial value but also provide us an insight into modern society and this new generation. In this project, we will employ a data driven method to depict the revolution in music industry and try to predict its future."
  },
  {
    "objectID": "introduction.html#background-research-works-done-in-this-field",
    "href": "introduction.html#background-research-works-done-in-this-field",
    "title": "An Analysis of the Trend in Today’s Music Industry",
    "section": "Background: Research Works Done in this Field",
    "text": "Background: Research Works Done in this Field\nSince the great changes in musical industry, extensive researches were done in this realm, offering us the background knowledge and valuable insights. I shall address two important publications here which provide us important background knowledge and insight on understanding today’s music trend.\n\nPublication-1: Changing Their Tune: How Consumers’ Adoption of Online Streaming Affects Music Consumption and Discovery\nThis paper throughly described how the streaming service impact the way music were consumed and user behavior. The author segment the research into three parts, first they identify the increment contribution of streaming service is contributing to the overall music consumption. Secondly, it compare the diversity in the consumers’ choice in music categories between streaming service and traditional ownership model. Finally, it researched what could impact a song’s success in the streaming environment (Datta, Knox, and Bronnenberg 2018).\nTo analysis the effects of adopting streaming service on individual music consumption, they use an anonymous third party service tracking the consumer’s platform preference and their listening behavior to create a panel of data. By applying a difference in differences (DiD) method, the authors compare the consumption differences between the group of people who adopt the streaming service and people who did not. According to their findings, adopting to streaming lead to a 49% increase of music consumption across all the platform. Furthermore listeners in streaming service tend to have a wider spectrum of music choices. The author noted it is benefit to those smaller artist to get attention but it also make the situation more competitive since the listeners now tend to discover new music instead of staying still (Datta, Knox, and Bronnenberg 2018).\n\n\nPublication-2: Tastes and Age: A Study of the Relationship between Music Tastes and Age Cohorts\nIn this paper, the author tried to investigate the impact of ages’ influence on individual music preference. The main object is to discover the relationship between then contemporary music taste and age age cohorts (Glevarec, Nowak, and Mahut 2020). The data they used for research is the ELIPSS inquiry from 2013 conducted by the Centre of Socio-Political Data in Pairs. This enquires include 13 genres of music and with a sample space of 892. The research methodology employed a regression analysis on the socio-demographic variables including age, education, profession, gender of a person and their taste of music genres. The finding reveal that for most music genres, age of the listener is the dominate factor to determine whether it will be appreciate or not. It is also noticed that people tend to rate the artist higher when they are at a similar age with the listener. Also, the author went further and point out it can be inferred from the research that people are likely to be linked to an artist they encountered during their teenager (Glevarec, Nowak, and Mahut 2020).\n\n\nOther Relevent Works:\nThere are many other researches dive into the topic of the contemporary music trends in many perspective to provide us the background of this project. The trend can be split into three aspect, the trend for music content, the trend in music consumption and the trend in music creation.\n\nTrend in Music Contents:\nIn terms of music content, current researches indicating the lyric of the songs are getting simpler and the songs with simple lyrics tend to be more successful (Varnum et al. 2021). Also, according to research, the lyrics now more likely to contain negative emotions (Brand, Acerbi, and Mesoudi 2019). This is important to notice since the research done by Borg and Hokkanen shows that lyrics plays an significant role in a music’s success (Borg and Hokkanen 2011). There are also new genres emerging, Base on the second publication summary above, we can observe the new Z generation is the key audience of those novice genres (Datta, Knox, and Bronnenberg 2018). According to Klement & Strambach, the local and extra-local knowledge source contribute to those new genres creation (Klement and Strambach 2019).\n\n\nTrend in Music Consumption:\nThe advent of streaming services has totally revolutionized music consumption as describe in the first publication’s summary. Additionally, changes also reflect on the form music been consumed. Nowadays music is often consume in combine with video or games. An good example is the TikTok. According to study, the more interactive mode in platform like TikTok short video make people foster affliction by mixing the music with image, video and text, and it is also shown as a way to self-express and connect people with similar afflictions (Vizcaı́no-Verdú and Abidin 2022).\n\n\nTrend in Music Creation:\nThere are also revolution in music creations. With more AI assistance tools emerging, even individual without traditional musical training could create songs, however it also means the standard for hit song is increasing (Liu 2019). Also the streaming service like Spotify enable artist to upload their work without the need of signing to a label. However, these technology doesn’t always promote the creation. THOMAS HODGSON described it as ‘Creative Ambivalence’ mainly caused by the opaque nature of recommending algorithm (Hodgson 2021). Artist might spend more time to align their work to the algorithm rather than being creative (Hodgson 2021)."
  },
  {
    "objectID": "introduction.html#research-goal-and-hypothesis",
    "href": "introduction.html#research-goal-and-hypothesis",
    "title": "An Analysis of the Trend in Today’s Music Industry",
    "section": "Research Goal and Hypothesis",
    "text": "Research Goal and Hypothesis\nIn this project, our aim is to provide a comprehensive, data-driven analysis of contemporary music industry trends and potentially predict its future shifts. We hypothesize that the as the Generation Z becoming the main consumer, there would be significant changes regarding the genre, rhythm and lyric of trending music. We also expect to see a trend where individual artist and artists who also from Gen Z are thriving."
  },
  {
    "objectID": "introduction.html#data-science-questions",
    "href": "introduction.html#data-science-questions",
    "title": "An Analysis of the Trend in Today’s Music Industry",
    "section": "Data Science Questions",
    "text": "Data Science Questions\nTo test our hypothesis, we shall try to answer these data driven questions:\n\nMusic Contents:\n\nWhat are the most popular genres of music today?\nHow did the popular genres changed over the three decades and is the changing rate faster or not?\nAre the songs now more homogenized than before in terms of the rhythm, lyrics and genre?\nHas the linguistic diversity in trending music becoming greater or not?\nWhat are genre or artist attract most to Generation Z?\n\n\n\nMusic Consumption:\n\nDid the consuming behavior shifted for music listeners compare with before streaming service?\nWhat is the impact of the integration between music and other media type like game and videos?\nTo what extent has the music industry been impacted by the Covid?\nAre there any common traits for those songs been made into a Sped-up remixes?\nAbout the listeners of different genres or rhythms, are they highly clustered in some trait?\n\n\n\nMusic Creation:\n\nIs it easier now to gain fame as an independent artist since AI tools make it easier to produce high quality music?\nHas there been a noticeable trend in artists archive fame younger than before?"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "The data used for this project will be host on the Georgetown Domain which can be access here: data"
  },
  {
    "objectID": "data.html#data-hosting",
    "href": "data.html#data-hosting",
    "title": "Data",
    "section": "",
    "text": "The data used for this project will be host on the Georgetown Domain which can be access here: data"
  }
]